{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "LQDQfTDHght4",
        "YPmwlB0rzfQX",
        "g2JcPqdNoidF",
        "zHFQWF1ylK8v",
        "_alYSDYLn_iP"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2"
      ],
      "metadata": {
        "id": "HmPxslriaLBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente problema, se presenta un conjunto de datos correspondientes a escritos de Shakespear. El objetivo del problema es crear un modelo capaz de generar texto con dialecto de época y escritura en verso y prosa."
      ],
      "metadata": {
        "id": "LM9LGmdGf6S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerías y entorno"
      ],
      "metadata": {
        "id": "LQDQfTDHght4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow 2.15\n",
        "!pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "id": "TMk3TDzUVbzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a25e202-249b-4f9d-8aaf-95214dcba2a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.68.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.69 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRaa1rfYfHrK",
        "outputId": "94fd6655-1a83-4e85-a65a-09486297d886"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=36496eecf6561f1a7b39d44f80ef8381c392dda4d30338d9c7d5411f962275be\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.0\n",
            "    Uninstalling httpx-0.28.0:\n",
            "      Successfully uninstalled httpx-0.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.1.147 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.54.5 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from googletrans import Translator\n"
      ],
      "metadata": {
        "id": "2GnYMdQPgk7x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()"
      ],
      "metadata": {
        "id": "c1aq1OznfQV3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar para que TensorFlow utilice la GPU por defecto\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Configurar para que TensorFlow asigne memoria dinámicamente\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        # Especificar la GPU por defecto\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Manejar error\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "tvt87nZ6hl-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02368e3-ad86-41d4-b7f4-696d305bd28d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "dPvq7eUmhqgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVSjpQPGffP3",
        "outputId": "a111e9c8-67c6-41b4-ab7d-6701a7ad9773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descarga completada.\n"
          ]
        }
      ],
      "source": [
        "# URL del dataset\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "\n",
        "# Realizar la solicitud de descarga\n",
        "response = requests.get(url)\n",
        "\n",
        "# Guardar el archivo\n",
        "with open(\"shakespeare.txt\", \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(\"Descarga completada.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(\"shakespeare.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlpPa_JAhcaL",
        "outputId": "75b9b530-b1be-4e22-e33b-5ef3cc44a12e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 2000 characters in text\n",
        "print(text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh_HvXOshvfL",
        "outputId": "19db888d-ef1c-4e5d-8852-19afaf2e35f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbGQT5cQhx50",
        "outputId": "295685c1-6411-4a91-ef87-62f8e2be87c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo carácter a carácter"
      ],
      "metadata": {
        "id": "qXfJPqUJZ6O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento"
      ],
      "metadata": {
        "id": "RtabpoUciKSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorización"
      ],
      "metadata": {
        "id": "L2Hx3VrxiM0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previo al entrenamiento, necesitamos convertir el texto a una representacion numerica.\n",
        "\n",
        "La capa tf.keras.layers.StringLookup nos permite convertir cada caracter en un ID numerico. Solo necesita que el texto este separado primero en tokens."
      ],
      "metadata": {
        "id": "ZJ9FHbQhiQ3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora creamos la capa tf.keras.layers.StringLookup:"
      ],
      "metadata": {
        "id": "G3fbfbthiXHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "NKMGaya6iVur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos convierte de tokens a IDs de caracteres"
      ],
      "metadata": {
        "id": "FUuzbon4ifYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que el proposito de este ejercicio es generar texto, tambien sera importante invertir esta representacion y recuperar texto legible desde estos IDs. Para esto utilizamos `tf.keras.layers.StringLookup(..., invert=True).`\n",
        "\n",
        "Nota: Aquí, en lugar de pasar el vocabulario original generado con `sorted(set(text))`, usamos el método `get_vocabulary()` de la capa `tf.keras.layers.StringLookup` para que los tokens `[UNK]` se configuren de la misma manera."
      ],
      "metadata": {
        "id": "J9MJXrecij_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "uWtBS7u8ipTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta capa recupera los caracteres desde los vectores de IDs y los retorna como un `tf.RaggedTensor` de caracteres:\n",
        "\n"
      ],
      "metadata": {
        "id": "iOEeQXMEiq7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente usando `tf.strings.reduce_join` se pueden volver a juntar los caracteres en texto."
      ],
      "metadata": {
        "id": "yKU08XcuivoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "IrgdHsN1iz5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos el texto en ids numéricos\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipJx5M7tYqMU",
        "outputId": "d981bbc4-10b1-4f64-eda0-bec33cc35249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "MWOMmBI3YsFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eKtJpjZYvCt",
        "outputId": "9b80ff0c-ab15-475d-f6d5-76d454fd1b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción carácter a carácter\n",
        "\n",
        "\n",
        "Dado un caracter, o una secuencia de caracteres, ¿cuál es el siguiente caracter más probable? Esta es la tarea para la que estamos entrenando al modelo. La entrada al modelo será una secuencia de caracteres y entrenamos el modelo para predecir la salida: el siguiente carácter en cada paso de tiempo.\n",
        "\n",
        "Dado que los RNN mantienen un estado interno que depende de los elementos vistos anteriormente, a partir de todos los caracteres calculados hasta este momento, ¿cuál es el siguiente carácter?"
      ],
      "metadata": {
        "id": "XVOFU5DFi3c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear los ejemplos de entrenamiento\n",
        "\n",
        "Dividimos el texto en secuencias de ejemplo. Cada secuencia de entrada contendrá `seq_length` caracteres del texto.\n",
        "\n",
        "Para cada secuencia de entrada, los targets correspondientes contienen la misma longitud de texto, excepto que se desplazan un carácter hacia la derecha.\n",
        "\n",
        "Así que divida el texto en fragmentos de `seq_length+1`. Por ejemplo, digamos `que seq_length` es 3 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hol\" y la secuencia target \"ola\".\n",
        "\n",
        "Para hacer esto, usamos la función `tf.data.Dataset.from_tensor_slices` para convertir el vector de texto en una secuencia de índices de caracteres."
      ],
      "metadata": {
        "id": "e2FetSLejSWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "yP19-GoLja3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método `batch` nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado.\n",
        "\n"
      ],
      "metadata": {
        "id": "CbQf6NCLjdO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeVmQtC5jfHQ",
        "outputId": "7d71f014-587f-4f19-ed20-7d9c6cf039fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es más fácil ver lo que esta haciendo si unimos de vuelta los tokens en texto:"
      ],
      "metadata": {
        "id": "n4Fwj_3gjixc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3HBX96rjhkk",
        "outputId": "bb42cc25-88af-4736-de15-fe4ab27812f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el entrenamiento, necesitaremos un conjunto de datos de pares `(input, label)`. Donde `input` y `label` son secuencias. En cada timestep, la entrada es el carácter actual y la etiqueta es el siguiente carácter.\n",
        "\n",
        "Aquí hay una función que toma una secuencia como entrada, la duplica y la desplaza para alinear la entrada y la etiqueta para cada timestep:"
      ],
      "metadata": {
        "id": "xFUd5l16jnjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "92Zo4tvIjnMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "UlnUzZBkjsIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXpeYymLjtIf",
        "outputId": "0b4d192d-e905-4b71-fade-9916d1a54993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batches de entrenamiento"
      ],
      "metadata": {
        "id": "BMV2Jepcj5Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos `tf.data` para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, es necesario mezclarlos y batchearlos."
      ],
      "metadata": {
        "id": "0C08AkCoj_w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgn9pU-Rj40C",
        "outputId": "fa27b25a-e1ca-4721-ab25-852b0621696a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construccion del modelo\n",
        "\n",
        "\n",
        "Este modelo tiene tres capas:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: La capa de entrada. Una lookup table entrenable que asignará cada ID de carácter a un vector con dimensiones `embedding_dim`;\n",
        "* `tf.keras.layers.GRU`: una capa recurrente GRU de tamaño units=rnn_units (también se puede usar una capa LSTM aquí).\n",
        "* `tf.keras.layers.Dense`: La capa de salida, con salidas` vocab_size`. Genera un logit para cada carácter del vocabulario. Estas son las probabilidades de cada caracter según el modelo."
      ],
      "metadata": {
        "id": "is38o59lkEFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "9mVaflZYkLH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "YoE96w00kNNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "3cXZQsp5kPC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por cada caracter el modelo calcula su embedding, corre la GRU un timestep con el embedding como entrada y aplica la capa densa para generar los logits prediciendo la probabilidades del siguiente caracter."
      ],
      "metadata": {
        "id": "dGZ9YCZkkjZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probar el modelo\n",
        "\n",
        "Ejecutamos el modelo para ver que se comporta como se esperaba.\n",
        "\n",
        "Primero verificamos la shape de salida:"
      ],
      "metadata": {
        "id": "MMwHUDrQkoe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht3JUN0ykiw9",
        "outputId": "7953d051-18b3-4e63-b2ba-576cf486dca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejemplo anterior, la longitud de la secuencia de la entrada es 100, pero el modelo se puede ejecutar con entradas de cualquier longitud:"
      ],
      "metadata": {
        "id": "NehQq2FFku7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fpP2A43kuRz",
        "outputId": "558cd8e0-67cf-4118-fc22-4a72bf65e063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para obtener predicciones reales del modelo, se deben tomar muestras de la distribución de salida para obtener índices de caracteres reales. Esta distribución está definida por los logits sobre el vocabulario de los caracteres.\n",
        "\n",
        "Nota: Es importante tomar una muestra de esta distribución, ya que tomar el argmax de la distribución puede fácilmente hacer que el modelo se atasque en un bucle.\n",
        "\n",
        "Tomando como ejemplo el primero del batch:"
      ],
      "metadata": {
        "id": "eoeyrEz5kydf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "b95l1tOPk0F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos da para cada timestep una predicción del siguiente índice de caracteres:\n",
        "\n"
      ],
      "metadata": {
        "id": "nrMdexg4nMnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R66EvZVonNun",
        "outputId": "e6715fee-1228-4da9-fe8e-3d9a0f38a2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14, 32, 42, 29, 10,  2,  2,  6, 50, 22, 18,  4, 32, 45, 63,  5, 52,\n",
              "       41, 37, 32, 62, 52, 57, 44, 15, 57, 61, 57, 61,  5, 20,  7, 15, 59,\n",
              "       58, 59, 29, 12, 12, 43, 37, 41, 35, 13, 40, 33, 30, 16,  7, 58, 34,\n",
              "       16, 37, 50, 58, 32,  7, 19,  9, 42, 48, 31, 60, 62, 20,  1, 12,  3,\n",
              "       53, 47, 45, 34, 50, 11, 14,  1, 11, 33, 37, 41,  8, 48, 62, 23,  4,\n",
              "       54, 28, 18, 55,  6, 31, 16, 19, 19, 29,  9, 40, 55, 52,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "El problema puede tratarse como un problema de clasificación estándar. Dado el estado RNN anterior y la entrada en este timestep, predice la clase del siguiente carácter.\n"
      ],
      "metadata": {
        "id": "kw5ZlVtIob9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Agregamos un optimizador y una funcion costo**\n",
        "\n",
        "La función de pérdida estándar `tf.keras.losses.sparse_categorical_crossentropy` funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
        "\n",
        "Debido a que su modelo devuelve logits, necesita configurar el indicador `from_logits`."
      ],
      "metadata": {
        "id": "I3i-p3PEbERo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "id": "FWNuzPqjobuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "7jnisWXqolb6",
        "outputId": "feb29bf7-f66a-4a59-fba9-02bb20eb3a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1898994, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo recién inicializado no debería estar demasiado seguro de sí mismo, todos los logits de salida deberían tener magnitudes similares. Para confirmar esto, puede comprobar que la exponencial del costo medio es aproximadamente igual al tamaño del vocabulario. Una pérdida mucho mayor significa que el modelo está seguro de sus respuestas incorrectas y está mal inicializado:"
      ],
      "metadata": {
        "id": "LgCLUMoubS-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr2No2FpbUGi",
        "outputId": "68aaabcc-f2ab-4c4a-a462-c9c8436b661f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.01615"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos el modelo con `tf.keras.Model.compile` indicando el optimizador y la funcion costo:\n",
        "\n"
      ],
      "metadata": {
        "id": "8fo1BV4LbWc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "67r3M9hKbWEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoints del modelo**"
      ],
      "metadata": {
        "id": "SnEk5zVDbhrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos el callback `tf.keras.callbacks.ModelCheckpoint` para que se guarden checkpoints del modelo durante el entrenamiento."
      ],
      "metadata": {
        "id": "OTg6MiVccSwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "0TSvqyQNbce5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejecucion del entrenamiento**"
      ],
      "metadata": {
        "id": "PvM3EAr-cWcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero entrenamos con 20 épocas, al no obtener buenos resultos, decimos utilizar 40.\n",
        "\n",
        "En el entrenamiento con 20 épocas notamos que el texto no era coherente."
      ],
      "metadata": {
        "id": "Vz-Uh1V0cYCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40"
      ],
      "metadata": {
        "id": "wXZGh9tOcZz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n"
      ],
      "metadata": {
        "id": "dNF_LWTRgjs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmTBQfVvYiru",
        "outputId": "486a1e0c-fd2d-4fc1-b4ed-f4446f748b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 10s 43ms/step - loss: 2.7180\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 9s 42ms/step - loss: 1.9870\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 1.7037\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 8s 41ms/step - loss: 1.5431\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 1.4449\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 1.3771\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 1.3238\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 8s 43ms/step - loss: 1.2794\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 8s 43ms/step - loss: 1.2378\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 1.1978\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 9s 42ms/step - loss: 1.1571\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 9s 42ms/step - loss: 1.1152\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 9s 42ms/step - loss: 1.0710\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 1.0242\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 9s 44ms/step - loss: 0.9749\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.9227\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.8698\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 8s 43ms/step - loss: 0.8169\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 8s 43ms/step - loss: 0.7643\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.7171\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.6728\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 8s 43ms/step - loss: 0.6330\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 0.5985\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.5674\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 0.5418\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 0.5217\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 0.5031\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 8s 43ms/step - loss: 0.4870\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.4769\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 0.4682\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.4602\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.4471\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.4419\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.4361\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.4319\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 0.4260\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 0.4288\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 8s 42ms/step - loss: 0.4242\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 9s 42ms/step - loss: 0.4200\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 9s 42ms/step - loss: 0.4183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generacion de texto"
      ],
      "metadata": {
        "id": "ehQCTn5hcdk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La forma más sencilla de generar texto con este modelo es ejecutarlo en un bucle y realizar un seguimiento del estado interno del modelo a medida que lo ejecutamos.\n",
        "\n"
      ],
      "metadata": {
        "id": "0XEMqJ5jciqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada vez que llamamos al modelo, pasamos algún texto y un estado interno. El modelo devuelve una predicción para el siguiente caracter y su nuevo estado. Vuelva a pasar la predicción y el estado para continuar generando texto.\n",
        "\n",
        "\n",
        "Lo siguiente hace una predicción de un solo paso:"
      ],
      "metadata": {
        "id": "Pmxx-nvnck3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "6dwUVnTtccq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "EfeIXkqqdHZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo ejecutamos en un bucle para generar texto. Al observar el texto generado, vemos que el modelo sabe cuándo poner mayúsculas, hacer párrafos e imita un vocabulario de escritura similar al de Shakespear. Con el reducido número de épocas de entrenamiento, todavía no ha aprendido a formar frases coherentes, incluso primero probamos con 20 épocas y luego con 40 y notamos el mismo compartamiento."
      ],
      "metadata": {
        "id": "ByXqYgsDdJPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['To be or not to be'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI38ZqQ5dJDA",
        "outputId": "a01c5062-85ba-4104-e7aa-625903722f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be the office, yet his sure\n",
            "With such a disinised against me one\n",
            "Of what leave the servants of my mind\n",
            "That hath brought so much sectars by your grace.\n",
            "Still, good my lord, let me entreat of her\n",
            "our father wear by a house of Marcius.\n",
            "\n",
            "FRIAR PHEYCUS:\n",
            "He'll prove a ted-mer you.\n",
            "\n",
            "BENVOLIO:\n",
            "Tut, you say is that these stinks of traging tongue?\n",
            "\n",
            "GLOUCESTER:\n",
            "The gods grant that spare and fled to give him gentlemen,\n",
            "The leaves and frankings on others,\n",
            "Therefore this other fling is not the stroke\n",
            "And this fash-bold's virtue. Will you go along?\n",
            "\n",
            "POLIXENES:\n",
            "Keep at me! 'she'er, madam:\n",
            "Against what man thou had, do make the crown;\n",
            "Which are unavoiding in his sight, lords,\n",
            "Let him not die, tranio. I like your ladyship\n",
            "To use it as a persetual rock,\n",
            "Which let their person proson bid gamm from him were,\n",
            "Tell him with Richmond's worthy days again, Rubles.\n",
            "\n",
            "KING LEWIS XI:\n",
            "Warwick, what are you? were there give the nort? their lates\n",
            "should Hortensio.\n",
            "\n",
            "POMPEY:\n",
            "Why,\n",
            "As all turns a deel over-proof, let him c \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 11.43207836151123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder interpretar mejor los resultados obtenidos, lo traducimos al español. La idea es poder analizar si el texto generado tiene coherencia."
      ],
      "metadata": {
        "id": "l3QXkYukfis2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traducción al español\n",
        "generated_text = result[0].numpy().decode('utf-8')\n",
        "translated_text = translator.translate(generated_text, src='en', dest='es').text\n",
        "print(\"Texto en español:\\n\")\n",
        "print(translated_text, '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgjExUiBfgG2",
        "outputId": "fb20b0d7-9047-4ba2-cf70-e014d79d610b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto en español:\n",
            "\n",
            "Ser o no ser la oficina, sin embargo, su seguro\n",
            "Con tan desinyado contra mí uno\n",
            "De lo que deja a los sirvientes de mi mente\n",
            "Eso ha traído tantos sectares por su gracia.\n",
            "Aún así, bien mi señor, déjame suplicarla\n",
            "Nuestro padre usa junto a una casa de Marcius.\n",
            "\n",
            "Fray Pheycus:\n",
            "Él demostrará un Ted-Mer tú.\n",
            "\n",
            "Benvolio:\n",
            "Tut, ¿dices que estos apestan la lengua trragante?\n",
            "\n",
            "Gloucester:\n",
            "Los dioses conceden ese repuesto y huyeron para darle caballeros,\n",
            "Las hojas y los franqueos en los demás,\n",
            "Por lo tanto, esta otra aventura no es el accidente cerebrovascular\n",
            "Y la virtud de este boquiabierto.¿Vas a ir?\n",
            "\n",
            "Polixenes:\n",
            "¡Sigue conmigo!'Ella es, señora:\n",
            "Contra lo que tenías, haz la corona;\n",
            "Que son inevitantes en su vista, Señores,\n",
            "No lo muera, Tranio.Me gusta tu señoría\n",
            "Para usarlo como una roca persetual,\n",
            "Que dejó que su persona Proson Bid Gamm de él fuera,\n",
            "Dígale con los días dignos de Richmond nuevamente, rublos.\n",
            "\n",
            "Rey Lewis XI:\n",
            "Warwick, ¿qué eres?¿Hubo dando al Nort?sus laces\n",
            "debería hortensio.\n",
            "\n",
            "POMPEYO:\n",
            "Por qué,\n",
            "Mientras todo gira una deel en exceso, déjelo C \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consideramos que el modelo capta bien la estética y el estilo de Shakespeare pero necesita mejoras para generar contenido más coherente y significativo.\n",
        "Probablemente la longitud de secuencia utilizada durante el entrenamiento influye en la capacidad del modelo para mantener la coherencia.\n",
        "\n",
        "A continuación probaremos con distantes temperaturas y logitudes de secuencia."
      ],
      "metadata": {
        "id": "7JlkLf67dMcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos que el modelo genere texto *más rápido*, lo más fácil que se puede hacer es generar el texto por batches. En el siguiente ejemplo, el modelo genera 5 resultados aproximadamente en el mismo tiempo que tomó generar 1 arriba."
      ],
      "metadata": {
        "id": "hTsztEtbdOiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['To be or not to be', 'To be or not to be', 'To be or not to be', 'To be or not to be', 'To be or not to be'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dsmP_1ddQEs",
        "outputId": "1c39586f-4f54-48ae-8aa9-219b4ba6f39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"To be or not to be a luast: yet stands at night.\\nGod save the king! both powerful partner rife,\\nTwoughts thy over-lean; then let me have I see\\nthyself, friends, Gremits three great ones.\\n\\nFirst Murderer:\\nAnve me, one Moring, of all kings enforced\\nTo gass a gentle and forswearing at\\nSome shape and mortal steel, to want\\nThe ugleress of hot only.\\n\\nLEONTES:\\nThou dost, and joid, my faith, for a\\nnot that such sorrow buy why now ere I could see\\nBut by Angelo is an honour that must\\nDid I lay that live so hard hither. By your regiment.\\n\\nDORCAS:\\nMen of Gloucester's death, for my defe girly, call home,\\nGod will I stay, the ship spoke of Lancaster.\\nBut, stay my throne, and better end o'er-dry them;\\nWho, all this while he is affairs male.\\nCome away; commend me to London,\\nDo thought withal, and in her good cause\\nBut but a schoolmaster with horses' us, for east\\nWill prove the function of my brother:\\nAnd why she's care for King of Hercules; beholdst kind\\nwishing the forfeit of my mind.\\n\\nLADY ANNE:\\nVouchsafe together ma\"\n",
            " b\"To be or not to be a lawful sorrow.\\n\\nYORK:\\nO Goodly boy! stone shall I send to follow\\nHow no remedies fair princes;\\nO, do not pass to keep your men:\\nBut say, what doth he that are true,\\nAnd threat the afternoon of constant is!\\n\\nBUCKINGHAM:\\nRather:\\n'Tis like your lass to exto my wife.\\n\\nPOMPEY:\\nBoy, this state, and go you to my noble honour's\\nwide-stay; winged time with unstrange to famish,\\nUntimely him repair nor tander, sings.\\nFor high revenge; but in an ass'd now\\nDoth grief hath best am come and work-dear\\nespiaped that upon my tresh and sparkles hate our freshes\\nBy ear to thus can find it right,\\nO'er laid to her friend natural thunder;\\nOur pernibitagory and his country, proud many\\nglad our bones.\\n\\nCOMINIUS:\\nHe longs upon him.\\n\\nDUKE VINCENTIO:\\nNay, take the mointion of your own breath;\\nFor thou wilt have my wit sundet o' the statualenets, and an old\\nthrone with the shepherd: so thrived arive,\\nI dare no worse than France:\\nFrom whence shall Wert both to answer thee,\\nWhere should square the murderous place\"\n",
            " b\"To be or not to be so notion.\\n\\nMIRANDA:\\nO, sir,\\nYour drumble in your stones Part of your follower,\\nYour father's house dull undertake to lime.\\n\\nHENRY BOLINGBROKE:\\nI spy like on Bianca mark a corsper.\\n\\nKATHARINA:\\nWhat a boy?\\n\\nBoth:\\nTurn, sweet saint, if he were standers off.\\n\\nFirst Citizen:\\nFor what, on his mercy! why the news is looker. Let them take\\nThe man in sall; and nothing can yield a\\nHumbirncious instigating in himself,\\nSpicition for them. Master, you shall stay within thieve,\\non no father; therefore, in King Edward's wife,\\n'right to death. He's hoars\\nMay do and get before such pates.\\n\\nJULIET:\\n\\nThird Conspirator:\\nSir, he is dead; when you shall retire.\\n\\nTRANIO:\\nFear a wench him with access of virtue,\\nThe greatures there will else their way with him.\\n\\nTYBALT:\\nUpon my life, Petruchio modest.\\n\\nSLY:\\nA' scandal'd your father's bosom, close you the\\nshoulder'd agreet; but this sins and starks\\nThe state of killy's eye, are weigh,\\nOur general appear, unless he speaks not how his fair\\nday, but it custimes \"\n",
            " b\"To be or not to be my heir; and my good fears' hands;\\nFor 'tis thy proud intelligence,\\nNot like a thankling, they under fetter he.\\n\\nHENRY BOLINGS:\\nBut had he took so will, that name is death.\\n\\nOfficer:\\n\\nLords':\\nBut is there any?\\n\\nISABELLA:\\nYes, brother, here it is.\\nRomen vaugh on them both to part them.\\n\\nLUCIO:\\n\\nISABELLA:\\nMost misery, if he had been took the bride\\nOf coming stain'd the king's officer captiance\\nHis discourse hath dishonour'd by the earth\\nAnd leave the name of Gloucester's death,\\nHis almies are much better than an east,\\nHast thou been meek, where the noblest children\\nWhich they have tripudent or power?\\nRomeo, this state, what shall I have tilled me?\\nNo, I'll take her lord of my tale about her;\\nAnd here's exactine in the viterin young's death,\\nI stay to their bits my present aid.\\nBut, to dive, my good lord, more counsellor's war:\\nThe banish'd multitude, or an oath branches\\nIn honour and a half and mine,\\nWho is already here in Vienna,\\nWhere shall have your father doth excused?\\n\\nFRIAR JOHN:\\n\"\n",
            " b\"To be or not to bear a woman.\\n\\nLEONTES:\\nWhat cannot make\\nsaid too. Come, enseeth let's from them.\\n\\nJULIET:\\nThe tongue's cousin, but that he is above\\nMessengers is the flaints and forwardness o' the crown,\\nAnd with the duilty dimpherous tongue,\\n'Alliff I am accused in them take still, and quite kings\\nsuch as soft to his country that we fear,\\nAnd the most voice of his grave imment,\\nThe bridegroom of sign tribunes, turnally,\\nAs gave their lips we stand on sharp,\\nIs note own lightness, such as is a worm.\\n\\nHENRY BOLINGBROKE:\\nIn God's name, sir; foolish words.\\n\\nPETRUCHIO:\\nPatience, ho! Angelo! For this unmusicalmast!\\nIs broke-night since you even here in another father;\\nEart waked by 'twere it so; even there,\\nI do not know the law: the heartuness\\nOf all seized so clurch and night: but ere\\nthings needs not here, but you: I spy sacred back\\nWhere Claudio stood'd before and accident. What\\nshall it beneigned out?\\n\\nBUCKINGHAM:\\nAre you a flesh whose bidds must buy and last?\\n\\nKING RICHARD III:\\nGood news have spoken t\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 11.744194030761719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del modelo caracter a caracter"
      ],
      "metadata": {
        "id": "bCigSVCqgcty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de temperaturas y longitudes\n",
        "temperaturas = [0.5, 1.0, 1.5]  # Baja, estándar y alta\n",
        "longitudes = [50, 100, 200]  # Fragmentos cortos, medianos y largos\n",
        "num_fragments = 5  # Número de fragmentos por configuración\n",
        "\n",
        "# Generar fragmentos\n",
        "generated_fragments = []\n",
        "for temp in temperaturas:\n",
        "    for length in longitudes:\n",
        "        print(f\"\\n--- Temperatura: {temp}, Longitud: {length} ---\\n\")\n",
        "\n",
        "        # Actualizar temperatura del modelo\n",
        "        one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=temp)\n",
        "\n",
        "        # Generar fragmentos para esta configuración\n",
        "        for i in range(num_fragments):\n",
        "            states = None\n",
        "            next_char = tf.constant([\"To be or not to be\"])  # Texto inicial\n",
        "            result = [next_char]\n",
        "\n",
        "            # Generar texto con longitud específica\n",
        "            for _ in range(length):\n",
        "                next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "                result.append(next_char)\n",
        "\n",
        "            # Convertir a cadena y guardar\n",
        "            text_new = tf.strings.join(result).numpy()[0].decode('utf-8')\n",
        "            generated_fragments.append((temp, length, text_new))\n",
        "            print(f\"Fragmento {i+1}:\\n{text_new}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0oiSUjuuOq2",
        "outputId": "6197d811-30d2-4c31-8a74-4c84fc4b45fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Temperatura: 0.5, Longitud: 50 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be a suitor?\n",
            "\n",
            "MERCUTIO:\n",
            "Nay, one sit in hope.\n",
            "\n",
            "GREMI\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be a suitor to my fear,\n",
            "And many an old man's enemie\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be a suitor?\n",
            "\n",
            "MERENIUS:\n",
            "Hear me, people!\n",
            "\n",
            "EDWARD:\n",
            "Bu\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be a popty to him.\n",
            "\n",
            "KING HENRY VI:\n",
            "Warwick, speak an\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be a suitor?\n",
            "\n",
            "MERCUTIO:\n",
            "Nay, I'll conduct his cousin\n",
            "\n",
            "\n",
            "--- Temperatura: 0.5, Longitud: 100 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be a king'ed out?\n",
            "\n",
            "BUCKINGHAM:\n",
            "My lord, I swear to thee say amen.\n",
            "I had a Raughty house, thy father Yo\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be a solencer sound;\n",
            "The tendering of all the world I am not for you.\n",
            "\n",
            "KATHARINA:\n",
            "They be it so.\n",
            "\n",
            "DUKE\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be a poor knave in her toward Gelly;\n",
            "And yet we should, unless the duty throughly,\n",
            "With a dogry with t\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be a pupil of report\n",
            "him. For a letter and my soul!\n",
            "Think what you might cry, we are in repual'd\n",
            "And m\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be a pupil: put in the norther too:\n",
            "If she be not away, that might will undertake to thee;\n",
            "Had I not r\n",
            "\n",
            "\n",
            "--- Temperatura: 0.5, Longitud: 200 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be a suitor to my fear.\n",
            "\n",
            "LEONTES:\n",
            "How!\n",
            "\n",
            "CORIOLANUS:\n",
            "Ay, marry, sir, that you swear to us!\n",
            "\n",
            "BENVOLIO:\n",
            "Two of the man come now, what hold it from\n",
            "darthwert: but, by God's mother and my friends,\n",
            "Yet thus f\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be so noble;\n",
            "It is no sight; I know it well.\n",
            "I swear to thee again: I must have put\n",
            "The fire and nobly often better.\n",
            "\n",
            "AUFIDIUS:\n",
            "Insulter's wedded,\n",
            "Namely through the dull and likelihood of breath,\n",
            "To ex\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be a suitor to my friends?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Had he not conquer of some private friends!'\n",
            "\n",
            "First Citizen:\n",
            "He's sentenced the former time.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "But thou didst vent our place:--why 'good let's \n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be a populous to lechery was this father;\n",
            "But only lost are to break o'ercharged that your friends:\n",
            "The sweetest or an ague before the stews,\n",
            "And make worship with their height before.\n",
            "\n",
            "HORTENSIO:\n",
            "Fear \n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to bear him now again.\n",
            "\n",
            "LUCIO:\n",
            "I grant yourself.\n",
            "\n",
            "WARWICK:\n",
            "Be Duke of Flance, but one dead wives, turns dead;\n",
            "And so he may better, and make in another's anchors,\n",
            "Our and a story sparely to the sin,\n",
            "It sha\n",
            "\n",
            "\n",
            "--- Temperatura: 1.0, Longitud: 50 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to bear the bad from thence:\n",
            "But is your fair prosperou\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to bear a both of all,\n",
            "And made an hand of mine too lam\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be a woman's top.\n",
            "This is a coddict trees had won;\n",
            "I\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be a poor knave to like:\n",
            "Richard, drest that summers\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to bear a person.\n",
            "\n",
            "CLARENCE:\n",
            "O, what foul play 'He kill\n",
            "\n",
            "\n",
            "--- Temperatura: 1.0, Longitud: 100 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be tempted bring\n",
            "And stay to pack again.\n",
            "\n",
            "PETRUCHIO:\n",
            "Padua adain; but, at least,\n",
            "Their enemy shore, do\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be her bridegroom commandman:\n",
            "Away! the tribunes are the cause why husband's fiends\n",
            "And weapons with h\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be a ludy's wife.\n",
            "\n",
            "KING RICHARD III:\n",
            "Be it born to be; and therefore pardon me,\n",
            "indeed, let thee conve\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be Lord Angelo?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sir, your magestical way to our house of your cloeds;\n",
            "And in my head \n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be a puison for me;\n",
            "Which in a cravest throne with one of us\n",
            "And Montagues and Henry's replied: the ti\n",
            "\n",
            "\n",
            "--- Temperatura: 1.0, Longitud: 200 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be condemn'd;\n",
            "We have a-dolmity of honour to the\n",
            "son, age, make their pastime at your highness' mouth:\n",
            "Therefore this Volumnable hour but my daughter?\n",
            "Is the axe men's ends may our Lovour's king?\n",
            "And th\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be a suitor? I must confirm none;\n",
            "We cannot without me: I say answer 'be!\n",
            "\n",
            "TYRREL:\n",
            "The citizens are married.\n",
            "\n",
            "SICINIUS:\n",
            "The grief she's bird intonce, in briefs,\n",
            "thereof is not she must have unlest you e\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to bear the game: the other grandent obsharing\n",
            "But in his voice, she's waked. We are not so.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why ever, Oxfidelons dagger what I a very journey\n",
            "In soul shamelling on; my woman:\n",
            "There doth \n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be absolved.\n",
            "\n",
            "Nurse:\n",
            "Take my path that he returning in mind\n",
            "Or forthwith to the fairness of his presence.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Which God, this state a hell and such a king,\n",
            "And he and 'tis unto my free con\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be executed in all rest,\n",
            "It shall be moon, or we togeth every\n",
            "meaning, if this deed. Even from beastly shapes,\n",
            "As change our steeds happine of your clester;\n",
            "But beggars I spake with hope, that it may kn\n",
            "\n",
            "\n",
            "--- Temperatura: 1.5, Longitud: 50 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be Holences; and that I wear the sea\n",
            "Was strong and \n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be touch'd with this dock, which in their powers\n",
            "Sha\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be sure to life myself:\n",
            "'fore learnimage may doth no\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be bonny mock'd ague for's true gone,\n",
            "when he shall \n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to bear abash Voluncious?\n",
            "-NORE:\n",
            "Thou didst permit\n",
            "That\n",
            "\n",
            "\n",
            "--- Temperatura: 1.5, Longitud: 100 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be hunter-dear!\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Hold on,\n",
            "poor citizens evant: come.\n",
            "What doy forbearing by this co\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be more and toes.\n",
            "\n",
            "RIVERS:\n",
            "Move the king is flight, the duty through the Tower,\n",
            "And all bound to the e\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be should chopher; if you seem them;\n",
            "Your Rome hath pass'd a venger landerous thing:\n",
            "Spoke laid o'e wa\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be my heir, must yield\n",
            "In botter presuments. The woe's toward her so very body,\n",
            "His lord Narly Picians\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be too nam; pray to you to my lord,\n",
            "Took pouce of high and mil-herminks from her,\n",
            "Took pains adieus, t\n",
            "\n",
            "\n",
            "--- Temperatura: 1.5, Longitud: 200 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to bear a king; and he\n",
            "kniths it restored against my king, and my sour himself\n",
            "Die with good Mather, there can never queen\n",
            "A spurable bratAling thio, or mostard,\n",
            "Nor tear your brother Vongets, each in his \n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be gone and potix,\n",
            "That runs, for ever frown in God's name? Jue hulk,\n",
            "Been baline chaste a paler plegaination become's voice\n",
            "Where Glougs quickly for his lovely breats:\n",
            "Your mercy diedment! my mistress'\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be Lucentio.\n",
            "\n",
            "LUCENTIO:\n",
            "Tranio, sweet, if there bees the world with shrond.\n",
            "Which, with it changed, in my faith in God's name! and, God,\n",
            "Where ever Kave the heid of our own reporated\n",
            "Before theirs.\n",
            "\n",
            "MEN\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to bear Luck'do was'd not fisher?\n",
            "What over that, speak'st thou would prevail be reported here;\n",
            "My name Gepose that after your lord he spidious,\n",
            "Free harves oath that loves high\n",
            "About her; strew'st the you\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be gone. Leave not in love,\n",
            "Even as they have agherp, signior, here's the world:\n",
            "Besides, upon thy vergea, thou comest in so tetuous me:\n",
            "I know him Dear in Rome: Judge,\n",
            "Romeo that killing; thou camest! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones modelo caracter a caracter"
      ],
      "metadata": {
        "id": "OSeiCzk9uf5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fragmentos más relevantes**\n",
        "\n",
        "\n",
        "\n",
        "1.   Temperatura: 0.5, Longitud: 200\n",
        "Fragmento:\n",
        "\n",
        "*To be or not to be a suitor?  \n",
        "MERCUTIO:  \n",
        "Nay, I'll come what I say, sir. I know this careful  \n",
        "man that want nothing for the posterns: these flesh  \n",
        "May be possessed with good and take in part  \n",
        "With peace thy wounds to the p*\n",
        "\n",
        "2.   Temperatura: 0.5, Longitud: 100\n",
        "\n",
        "\n",
        "*To be or not to be a suitor to my soul.  \n",
        "Canst thou not speak? O toward the shame of mine?  \n",
        "JULIET:  \n",
        "It is: and, in good*\n",
        "\n",
        "3. Temperatura: 1.0, Longitud: 200\n",
        "Fragmento:\n",
        "\n",
        "*To be or not to be? I'll take in the rock's death.  \n",
        "Now shall we do, if King Edward's friends,  \n",
        "But kills away: hence she is Warwick's  \n",
        "sweeting twought out of his chamber upon,  \n",
        "and not a proud here. Come away; let him ca*\n",
        "\n",
        "4. Temperatura: 1.5, Longitud: 100\n",
        "Fragmento:\n",
        "\n",
        "*To be or not to bear my nanks. Dors are  \n",
        "My friends what's lost I give my conscience,  \n",
        "Which by his head upon your pilgr*\n",
        "\n",
        "5. Temperatura: 1.5, Longitud: 200\n",
        "\n",
        "Fragmento:\n",
        "\n",
        "*To be or not to be a luave to-morrow,  \n",
        "Making and well obing.  \n",
        "\n",
        "KATHARINA:  \n",
        "A jest and sheken disdains to Rome.  \n",
        "\n",
        "HORTENSIO:  \n",
        "Father, be a Month'd, madam; see what I have seven years  \n",
        "He hath had, the very windows tongue.*\n"
      ],
      "metadata": {
        "id": "xHxlJZ8Gu9eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones:**\n",
        "\n",
        "\n",
        "1. Temperatura\n",
        "\n",
        "* Baja (0.5):\n",
        "La coherencia es alta, pero los textos tienden a ser menos creativos. Las frases son más predecibles y estructuradas\n",
        "\n",
        "* Media (1.0):\n",
        "Logra un equilibrio entre coherencia y creatividad. Los textos generados conservan el estilo de Shakespeare mientras permiten cierta flexibilidad en la composición.\n",
        "\n",
        "* Alta (1.5):\n",
        "Se observan más creatividad y palabras inventadas. Disminuye la coherencia\n",
        "\n",
        "2. Longitud\n",
        "* Fragmentos cortos (50):\n",
        "Mayor coherencia en frases individuales, pero menos desarrollo de ideas.\n",
        "\n",
        "* Fragmentos medianos (100):\n",
        "Mejor desarrollo de ides y coherencia.\n",
        "\n",
        "* Fragmentos largos (200):\n",
        "Más incoherentes en frases que tienen temperatura alta."
      ],
      "metadata": {
        "id": "Q1Jqm2Bsv2mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo palabra a palabra"
      ],
      "metadata": {
        "id": "hXuHYrgpjCJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "EacyY1yCYI_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL del dataset\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "\n",
        "# Realizar la solicitud de descarga\n",
        "response = requests.get(url)\n",
        "\n",
        "# Guardar el archivo\n",
        "with open(\"shakespeare.txt\", \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(\"Descarga completada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX_UHypgPMiN",
        "outputId": "3c9c2376-5d04-490d-dc3c-6d085fc5cfb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descarga completada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(\"shakespeare.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krSCx8r_PNyT",
        "outputId": "a766b0d0-a268-4375-f7ea-65f19932b1bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorización"
      ],
      "metadata": {
        "id": "93axB96OY_0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir el texto en palabras\n",
        "words = text.split()\n",
        "\n",
        "# Crear una capa para convertir palabras en IDs numéricos\n",
        "ids_from_words = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(set(words)), mask_token=None\n",
        ")\n",
        "\n",
        "# Crear una capa para convertir IDs numéricos de vuelta a palabras\n",
        "words_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_words.get_vocabulary(), invert=True, mask_token=None\n",
        ")\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(words_from_ids(ids), axis=-1, separator=' ')\n",
        "\n",
        "# Convertimos el texto en IDs numéricos a nivel de palabras\n",
        "all_ids = ids_from_words(words)\n",
        "print(f\"Total de palabras únicas: {len(ids_from_words.get_vocabulary())}\")\n",
        "\n",
        "# Crear el dataset a nivel de palabras\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "# Mostrar algunas palabras y sus IDs\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(words_from_ids(ids).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPU4b-4u5K8Q",
        "outputId": "0f0365da-7a8e-4fbb-992b-16c66029ee46"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palabras únicas: 25671\n",
            "First\n",
            "Citizen:\n",
            "Before\n",
            "we\n",
            "proceed\n",
            "any\n",
            "further,\n",
            "hear\n",
            "me\n",
            "speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción"
      ],
      "metadata": {
        "id": "nId0pqqwicIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batches de entramiento"
      ],
      "metadata": {
        "id": "TsFBdkuG6JhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la longitud de las secuencias (en palabras)\n",
        "seq_length = 20  # Por ejemplo, 20 palabras por secuencia\n",
        "\n",
        "# Agrupar las palabras en secuencias de longitud fija\n",
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "# Mostrar una secuencia de palabras como verificación\n",
        "for seq in sequences.take(1):\n",
        "    print(\"Secuencia completa:\", text_from_ids(seq).numpy().decode('utf-8'))\n",
        "\n",
        "# Función para dividir la secuencia en entrada (input) y objetivo (target)\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]  # Todo menos la última palabra\n",
        "    target_text = sequence[1:]  # Todo menos la primera palabra\n",
        "    return input_text, target_text\n",
        "\n",
        "# Crear el dataset de entrada y objetivo\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Mostrar un ejemplo de entrada y objetivo\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input  :\", text_from_ids(input_example).numpy().decode('utf-8'))\n",
        "    print(\"Target :\", text_from_ids(target_example).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpgZuAqF6MJU",
        "outputId": "7e087ef6-7bf4-4780-d5ab-c55acb98b597"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secuencia completa: First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to\n",
            "Input  : First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather\n",
            "Target : Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamos el modelo palabra a palabra"
      ],
      "metadata": {
        "id": "ki9vcoKSijov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construcción del modelo"
      ],
      "metadata": {
        "id": "DgLj6E816Sb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Longitud del vocabulario basado en palabras\n",
        "vocab_size_words = len(ids_from_words.get_vocabulary())\n",
        "\n",
        "# Dimensión del embedding (puedes mantenerlo igual o ajustarlo si lo consideras necesario)\n",
        "embedding_dim_words = 256\n",
        "\n",
        "# Número de unidades en la RNN\n",
        "rnn_units_words = 1024\n"
      ],
      "metadata": {
        "id": "OIMdmF1E8ABJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo palabra a palabra\n",
        "class WordLevelModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "LJwykPzy8DtX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar el modelo palabra a palabra\n",
        "model_2 = WordLevelModel(\n",
        "    vocab_size=vocab_size_words,\n",
        "    embedding_dim=embedding_dim_words,\n",
        "    rnn_units=rnn_units_words\n",
        ")\n"
      ],
      "metadata": {
        "id": "18_uAY2N8FYp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resumen del modelo\n",
        "model_2.build(input_shape=(None, None))  # Define el tamaño de entrada como (batch_size, seq_length)\n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIguiVU98HOD",
        "outputId": "124f680f-a289-4b9e-a62e-0faae8eac4ed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"word_level_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  6571776   \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  26312775  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36822855 (140.47 MB)\n",
            "Trainable params: 36822855 (140.47 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probamos el modelo"
      ],
      "metadata": {
        "id": "JwfW--r88Lqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar el dataset en lotes para garantizar que se pase correctamente al modelo\n",
        "BATCH_SIZE = 64\n",
        "dataset_batched = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Probar el modelo palabra a palabra\n",
        "for input_example_batch, target_example_batch in dataset_batched.take(1):\n",
        "    example_batch_predictions = model_2(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size_words)\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvWaJi2_8vhF",
        "outputId": "e5939ac9-d2b3-4cd0-edaf-dbe1517bbf4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 20, 25671) # (batch_size, sequence_length, vocab_size_words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar el modelo palabra a palabra\n",
        "for input_example_batch, target_example_batch in dataset_batched.take(1):\n",
        "    example_batch_predictions = model_2(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size_words)\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1-pUm2k8LLL",
        "outputId": "67ff543c-eed9-40b0-9b16-21772536b3b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 20, 25671) # (batch_size, sequence_length, vocab_size_words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo\n"
      ],
      "metadata": {
        "id": "tnP0iy1V83yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar la pérdida para el modelo palabra a palabra\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "hx-1g_Gz83Nd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la pérdida inicial para verificar que todo esté bien\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size_words)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "print(\"Perplexity:       \", tf.exp(example_batch_mean_loss).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwyFB4Ij9RIQ",
        "outputId": "7fe7453e-8a36-4270-832c-aae25825391b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 20, 25671)  # (batch_size, sequence_length, vocab_size_words)\n",
            "Mean loss:         tf.Tensor(10.153151, shape=(), dtype=float32)\n",
            "Perplexity:        25671.854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "ckiTSeMa9TNv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar la carpeta de checkpoints\n",
        "checkpoint_dir = './training_checkpoints_word_level'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "dS5cwo0t9VtK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir número de épocas para el entrenamiento\n",
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "fab3fcxq9YFG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo palabra a palabra\n",
        "history = model_2.fit(\n",
        "    dataset_batched,  # Usamos el dataset en lotes\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmWrLlzD9aW1",
        "outputId": "012a3e7d-12c0-43c6-e243-812c9046cfb7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "150/150 [==============================] - 13s 75ms/step - loss: 8.5576\n",
            "Epoch 2/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 7.5345\n",
            "Epoch 3/20\n",
            "150/150 [==============================] - 3s 23ms/step - loss: 7.1653\n",
            "Epoch 4/20\n",
            "150/150 [==============================] - 3s 23ms/step - loss: 6.8151\n",
            "Epoch 5/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 6.5047\n",
            "Epoch 6/20\n",
            "150/150 [==============================] - 3s 23ms/step - loss: 6.1728\n",
            "Epoch 7/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 5.8116\n",
            "Epoch 8/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 5.4333\n",
            "Epoch 9/20\n",
            "150/150 [==============================] - 3s 23ms/step - loss: 5.0589\n",
            "Epoch 10/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 4.6745\n",
            "Epoch 11/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 4.2670\n",
            "Epoch 12/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 3.9071\n",
            "Epoch 13/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 3.5850\n",
            "Epoch 14/20\n",
            "150/150 [==============================] - 3s 23ms/step - loss: 3.3117\n",
            "Epoch 15/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 3.0768\n",
            "Epoch 16/20\n",
            "150/150 [==============================] - 3s 23ms/step - loss: 2.8425\n",
            "Epoch 17/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 2.6535\n",
            "Epoch 18/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 2.4527\n",
            "Epoch 19/20\n",
            "150/150 [==============================] - 3s 23ms/step - loss: 2.2683\n",
            "Epoch 20/20\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 2.1179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de texto"
      ],
      "metadata": {
        "id": "ggWZ3xAniyxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStepWord(tf.keras.Model):\n",
        "    def __init__(self, model, words_from_ids, ids_from_words, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.words_from_ids = words_from_ids\n",
        "        self.ids_from_words = ids_from_words\n",
        "\n",
        "        # Crear una máscara para evitar generar \"[UNK]\"\n",
        "        skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Colocar un -inf en cada índice prohibido\n",
        "            values=[-float('inf')]*len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Igualar la forma al vocabulario\n",
        "            dense_shape=[len(ids_from_words.get_vocabulary())]\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convertir cadenas a IDs de palabras\n",
        "        input_words = tf.strings.split(inputs)  # Dividir por palabras\n",
        "        input_ids = self.ids_from_words(input_words).to_tensor()\n",
        "\n",
        "        # Ejecutar el modelo\n",
        "        # predicted_logits.shape es [batch, word, next_word_logits]\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                              return_state=True)\n",
        "\n",
        "        # Solo usar la última predicción\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "\n",
        "        # Aplicar la máscara de predicción: prevenir que se genere \"[UNK]\"\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Muestrear los logits de salida para generar IDs de palabras\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convertir de IDs de palabras a palabras\n",
        "        predicted_words = self.words_from_ids(predicted_ids)\n",
        "\n",
        "        # Retornar las palabras y el estado del modelo\n",
        "        return predicted_words, states\n"
      ],
      "metadata": {
        "id": "zo-pEAOG813r"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una instancia del generador palabra a palabra\n",
        "one_step_word = OneStepWord(model_2, words_from_ids, ids_from_words, temperature=1.0)\n",
        "\n",
        "# Entrada inicial\n",
        "start = tf.constant([\"To be or not to be\"])\n",
        "states = None\n",
        "\n",
        "# Generar texto palabra a palabra\n",
        "generated_text = [\"To be or not to be\"]  # Iniciar con la frase semilla\n",
        "for _ in range(50):  # Generar 50 palabras\n",
        "    next_word, states = one_step_word.generate_one_step(start, states=states)\n",
        "    generated_text.append(next_word.numpy()[0].decode('utf-8'))  # Decodificar la palabra\n",
        "    start = next_word  # Usar la última palabra generada como entrada\n",
        "\n",
        "# Unir las palabras generadas en una cadena completa\n",
        "print(' '.join(generated_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW-MVDkp-Bz-",
        "outputId": "50a9d3c6-d4d0-4a57-f87d-8218a0fa9ff6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be ready to Padua and weary CURTIS: My I may still lay at the rancour and as dear brother; 'tis no sleep private ten master, my joy do through my sharp and being a words: For I have all thee, a biting greyhound To live. First Servant: Why thus, not to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traducción al español\n",
        "generated_text_str = ' '.join(generated_text)  # Join the list elements into a single string\n",
        "translated_text = translator.translate(generated_text_str, src='en', dest='es').text\n",
        "print(\"Texto en español:\\n\")\n",
        "print(translated_text, '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTZwaBd--nmG",
        "outputId": "d41799a7-e685-4c19-ab48-9ff10111253b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto en español:\n",
            "\n",
            "Estar o no estar listo para Padua y Weary Curtis: mi aún puedo acostarme en el rencor y como querido hermano;'No hay sueño privado diez maestro, mi alegría lo hace a través de mi aguda y siendo una palabras: porque tengo todo, un galgo para vivir.Primer sirviente: ¿Por qué así, no \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluamos el modelo"
      ],
      "metadata": {
        "id": "AvUVyenK_Q3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de temperaturas y longitudes\n",
        "temperaturas = [0.5, 1.0, 1.5]  # Baja, estándar y alta\n",
        "longitudes = [10, 20, 50]  # Fragmentos cortos, medianos y largos (en palabras)\n",
        "num_fragments = 5  # Número de fragmentos por configuración\n",
        "\n",
        "# Generar fragmentos\n",
        "generated_fragments = []\n",
        "for temp in temperaturas:\n",
        "    for length in longitudes:\n",
        "        print(f\"\\n--- Temperatura: {temp}, Longitud: {length} ---\\n\")\n",
        "\n",
        "        # Actualizar temperatura del modelo\n",
        "        one_step_model_word = OneStepWord(model_2, words_from_ids, ids_from_words, temperature=temp)\n",
        "\n",
        "        # Generar fragmentos para esta configuración\n",
        "        for i in range(num_fragments):\n",
        "            states = None\n",
        "            next_word = tf.constant([\"To be or not to be\"])  # Texto inicial\n",
        "            result = [\"To be or not to be\"]  # Iniciar con la frase inicial\n",
        "\n",
        "            # Generar texto con longitud específica\n",
        "            for _ in range(length):\n",
        "                next_word, states = one_step_model_word.generate_one_step(next_word, states=states)\n",
        "                result.append(next_word.numpy()[0].decode('utf-8'))  # Decodificar palabra\n",
        "\n",
        "            # Convertir a cadena y guardar\n",
        "            text_new = ' '.join(result)  # Combinar palabras en un texto completo\n",
        "            generated_fragments.append((temp, length, text_new))\n",
        "            print(f\"Fragmento {i+1}:\\n{text_new}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmAd8GfN_PoF",
        "outputId": "fff36271-5e08-45b2-d2c7-42ec50b16c0a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Temperatura: 0.5, Longitud: 10 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be ingrate. TRANIO: 'Tis a man that I can seek my\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be ingrate. HORTENSIO: Padua and yet you are not in, the\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be ingrate. HORTENSIO: Padua and the first become that idle tears:\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be ingrate. HORTENSIO: Padua and entreat you to the Tower, Her\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be ingrate. TRANIO: 'Tis a man that I knew my hell,\n",
            "\n",
            "\n",
            "--- Temperatura: 0.5, Longitud: 20 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be ingrate. HORTENSIO: Padua and not prove a country, and how the use of thy breath to have a forfeit of\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be ingrate. PETRUCHIO: Hortensio, 'tis said, one that goes passing labour and the first complaint; the great man, Even to be\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be ingrate. GREMIO: So long by the Tower, my mirth, my son; That ever do not be. LUCENTIO: Go, call them\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be ingrate. HORTENSIO: Petruchio, fellow, 'tis not a fellow of my success. A contents of Norfolk, yet yet I mean to\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be ingrate. HORTENSIO: Padua and fetch the first complaint; effect a effect As I will accuse thee to my love. GREMIO:\n",
            "\n",
            "\n",
            "--- Temperatura: 0.5, Longitud: 50 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be rid of mine. HORTENSIO: Signior Baptista, is my good report to her my son And come to touch thy face? Their Buckingham, sir. ESCALUS: My lord, my lord. DUKE VINCENTIO: What, shall be my fortune and my fortune knows my father had a little man at the first head to\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be ingrate. TRANIO: 'Tis said, a man that apprehends death is ever a maid of a little man at home, and my mother, and leisure shall be my company. GREMIO: No, not the first that ever Ran on my side, and you shall add Myself and my father's year and cut\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be ingrate. HORTENSIO: Padua and entreat you away. LUCENTIO: Say that thou shalt ha' upon me: I will believe thee to the Tower, If our love is the air or the boar shall have thrust me to thrust me before mine own son, When ever I have done before you and\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be ingrate. HORTENSIO: But how now! what's the news? Signior CATESBY: My lord, she is but loud speak; The deed so fortune can me their female to my son, my father bears her honour and her and heir by her and that I have said, and so much more than my\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be rid of mine. TRANIO: What you shall have the Tower, and call her to be said, Signior Baptista, how I may go to go and to the end I have been much of my love. TRANIO: Nay, I will go without my wife. PAULINA: I am not of my heart\n",
            "\n",
            "\n",
            "--- Temperatura: 1.0, Longitud: 10 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be rid of the wench And how you head and cut\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be moon, And therefore within her, and all the man of\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be his, And so, she shall report so more ere I\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be he good. ISABELLA: O very sure, my wife, and give\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be rid our candle-holder, and children 't deliver, prepared, and therefore\n",
            "\n",
            "\n",
            "--- Temperatura: 1.0, Longitud: 20 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be new hour. SEBASTIAN: Ha, quoth man within me, go and to her. GRUMIO: Go not thy claim to go on\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be your long That you, Master VINCENTIO: You were upon Corioli do you bite thy company. GREMIO: Ay, go to me\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be cross to all my sight, swallowed your breath are in his blood. So he's a little news I would at\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be set away, Who far again; For she had as the fresh ear heard first to become the business can the\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be your brother's son, Was it is at heart. Sound, Alack, my poor craves a froward folks, all these mortal sun,\n",
            "\n",
            "\n",
            "--- Temperatura: 1.0, Longitud: 50 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be a mistress hath made a man so you again Than to blow you on thee to rain The sense of York, And ever believe two that whither? She is the head of man all earth and be An punish'd. ANGELO: What can we can not till my son, no, as\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be alone, so far 'tis sure, why dost thou wrong me, if you come to her and be pleased, how she is; For God, From her, then bring you, 'tis thus or as fair As if not bid all the rest, and thunder away. A prince he bids adventure What near\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be thus bold four and the common? SICINIUS: What cold and not so seen by the hated by war had ever sent to Rome, And all Italy. BAPTISTA: Let it dread news in mortal time: My honour and mine way till it in: And so I mean from home. a very\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be ingrate. HORTENSIO: Padua and go see do; and am I know my business in me; I am hearing of him till I have a servant to her here, And get thee ready to the world are not the forfeit of my love. GREMIO: Why, my Lord means to see my\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be in Padua and dog That am well than they are A course, though not so loath to one and man To see her horse doth use it From me, she else had been thus pleasant news, Then speak: Her die and height of thee and sought you in the effect\n",
            "\n",
            "\n",
            "--- Temperatura: 1.5, Longitud: 10 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be ingrate. TRANIO: Padua lived a subject, ho! spare duke, But\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be so. ESCALUS: We stoop about you and little world, To\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be no leisure to use her wooing this other task grieves\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be ingrate. is't worse; they done command. tongue it now, I'll\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be two moon, Did he without least peace, Even here's from\n",
            "\n",
            "\n",
            "--- Temperatura: 1.5, Longitud: 20 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be forced As sir, unto a hogshead. And therefore she shall find one keen hold at hand, both fill thee as\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be now! wedding king! shield it unto my heavens my house. What's glad you, as see this deadly duke, Such more\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be thine. TRANIO: Talk the bear Take these us, puling brother's aunt I night, as cunning doth be; Did only go\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be fine flourish'd God say 'tis Menenius, for Forgot it this safe need to her penitence: BAPTISTA: 'Tis rash: This eye\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be rid it so; nor spoken. villain! Well, brother away. ESCALUS: learned against you, James Fellow, BIONDELLO: A weak no, govern'd\n",
            "\n",
            "\n",
            "--- Temperatura: 1.5, Longitud: 50 ---\n",
            "\n",
            "Fragmento 1:\n",
            "To be or not to be fine Command our foul looking From AUFIDIUS: Would earth we too: child, my every should, This estimation when once this opens their only revenge and fellest throat, if twice passing well, And bring him eyes and God's good wife Than long lies away, at me, dead once fasting, and be\n",
            "\n",
            "Fragmento 2:\n",
            "To be or not to be half both both Who want at heart, with him! forted Uncle, I'll call them, how now! love's ample go far my mirth, my bears right my good my subject. KING EDWARD IV: Bear her my son, embrace into your sound so I'll hear, Henry, unfeigned come, go plain GLOUCESTER: ARIEL:\n",
            "\n",
            "Fragmento 3:\n",
            "To be or not to be gone, but very young A children bad, the filthy depend and hour can see what thou another, I that man I could hide, Well, course, Why Away with boon, He make a designs and exceed many First Senator: Why may Saint account till hadst princely end humble print or more\n",
            "\n",
            "Fragmento 4:\n",
            "To be or not to be away. PETRUCHIO: Now, head: I cannot thee; And, pretty fresh declines, and prove all sovereignty; on's presence Would comes. PETRUCHIO: Madam, me, what, forward, friend, he passing lord; O it becomes a shepherd's son; this is a falsehood to-morrow in those proud needy man Was four bewitch'd; me. But, here\n",
            "\n",
            "Fragmento 5:\n",
            "To be or not to be far Fit to them, so 'shrew As news to your fire, Who straight shall beat against my foul censure. ANGELO: Well, enter. ISABELLA: cousins, therein I may know what he hath till it seen again: Signior tempest man made now good, home, Say and fire thee loud If not, As\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones del modelo palabra a palabra\n"
      ],
      "metadata": {
        "id": "EJKlVpGykciK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fragmentos más relevantes**\n"
      ],
      "metadata": {
        "id": "zVymdsXJkoJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo 1:** Temperatura 0.5, Longitud 50\n",
        "--\n",
        "*To be or not to be rid of mine. HORTENSIO: Signior Baptista, is my good report to her my son And come to touch thy face? Their Buckingham, sir. ESCALUS: My lord, my lord. DUKE VINCENTIO: What, shall be my fortune and my fortune knows my father had a little man at the first head to...*\n",
        "\n",
        "El fragmento muestra alta coherencia en la estructura, con nombres de personajes y una narrativa que podría pasar como un diálogo del texto original. Sin embargo, repite estructuras como \"my fortune\" y pierde dirección en frases largas.\n",
        "\n",
        "---\n",
        "\n",
        "**Ejemplo 2:** Temperatura 1.0, Longitud 20\n",
        "---\n",
        "*To be or not to be your brother's son, Was it is at heart. Sound, Alack, my poor craves a froward folks, all these mortal sun.*\n",
        "\n",
        "Este fragmento es más diverso, con un uso interesante de términos como \"Alack\" y frases metafóricas (\"my poor craves a froward folks\"). Aunque algunas frases carecen de sentido completo, el tono y vocabulario son consistentes con el estilo.\n",
        "\n",
        "---\n",
        "\n",
        "**Ejemplo 3:** Temperatura 1.5, Longitud 50\n",
        "--\n",
        "*To be or not to be fine Command our foul looking From AUFIDIUS: Would earth we too: child, my every should, This estimation when once this opens their only revenge and fellest throat, if twice passing well, And bring him eyes and God's good wife Than long lies away, at me, dead once fasting, and be...*\n",
        "\n",
        "A pesar de la temperatura alta, se capturan palabras que parecen del estilo (\"AUFIDIUS\", \"fellest throat\"), pero el fragmento pierde coherencia.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "HuSrcgWfAFPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones generales"
      ],
      "metadata": {
        "id": "DIh0RyWblucT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coherencia**\n",
        "--\n",
        "* **Caracter a Carácter:**\n",
        "\n",
        "Mayor capacidad para generar frases con coherencia gramatical básica, ya que construye palabras a partir de caracteres y sigue patrones frecuentes aprendidos en el texto.\n",
        "Pierde coherencia narrativa en fragmentos largos y temperaturas altas, generando palabras inexistentes.\n",
        "\n",
        "---\n",
        "\n",
        "* **Palabra a Palabra:**\n",
        "\n",
        "Genera estructuras más coherentes a nivel narrativo, ya que utiliza palabras completas como base.\n",
        "Mantiene mejor el contexto en fragmentos largos, especialmente en temperaturas bajas y medias.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "El modelo palabra a palabra sobresale en coherencia gracias al uso de palabras completas.\n",
        "\n",
        "\n",
        "\n",
        "**Creatividad**\n",
        "--\n",
        "* **Caracter a Carácter:**\n",
        "Mayor diversidad en temperaturas altas debido a la construcción libre de palabras. Sin embargo, esto puede llevar a incoherencias o invenciones\n",
        "como \"luave\".\n",
        "\n",
        "---\n",
        "\n",
        "* **Palabra a Palabra:**\n",
        "Más restringido a palabras existentes del vocabulario, lo que reduce la creatividad en temperaturas bajas. En temperaturas altas, muestra combinaciones más inusuales, pero parece evitar palabras inventadas.\n",
        "\n",
        "----\n",
        "\n",
        "**Conclusión:**\n",
        "El modelo carácter a carácter genera más creatividad en términos de vocabulario, mientras que el modelo palabra a palabra es más conservador pero genera combinaciones interesantes en temperaturas altas.\n",
        "\n",
        "**Impacto de la Temperatura**\n",
        "--\n",
        "* **Temperatura Baja (0.5):**\n",
        "\n",
        "Ambos modelos generan textos más estructurados y repetitivos. Sin embargo, el modelo palabra a palabra mantiene mejor el estilo sin invenciones extrañas.\n",
        "\n",
        "* **Temperatura Media (1.0):**\n",
        "\n",
        "Ambos logran un equilibrio entre coherencia y creatividad. El modelo palabra a palabra genera textos narrativamente más ricos, mientras que el carácter a carácter conserva mejor el flujo rítmico.\n",
        "\n",
        "* **Temperatura Alta (1.5):**\n",
        "\n",
        "El modelo carácter a carácter pierde sentido rápidamente con palabras inventadas y narrativas desconectadas. El palabra a palabra conserva términos existentes, aunque las combinaciones son menos coherentes.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "\n",
        " La temperatura media es ideal para ambos modelos, pero el palabra a palabra es más estable en temperaturas altas.\n",
        "\n",
        "**Impacto de la Longitud**\n",
        "--\n",
        "* **Caracter a Carácter:**\n",
        "\n",
        "**Fragmentos cortos (50 caracteres):**\n",
        "\n",
        "Conserva coherencia en frases individuales.\n",
        "\n",
        "**Fragmentos medianos (100 caracteres):**\n",
        "\n",
        "Permite el desarrollo de ideas con cierto equilibrio.\n",
        "\n",
        "**Fragmentos largos (200 caracteres):**\n",
        "\n",
        "Pierde sentido rápidamente, especialmente en temperaturas altas.\n",
        "\n",
        "---\n",
        "\n",
        "**Palabra a Palabra:**\n",
        "**Fragmentos cortos (10 palabras):**\n",
        "Coherentes pero a veces demasiado simples.\n",
        "\n",
        "**Fragmentos medianos (20 palabras):**\n",
        "\n",
        "Logran el mejor equilibrio entre desarrollo narrativo y fluidez.\n",
        "\n",
        "**Fragmentos largos (50 palabras):**\n",
        "\n",
        "Mantienen el estilo, pero con un riesgo mayor de frases desconectadas en temperaturas altas.\n",
        "\n",
        "----\n",
        "\n",
        "**Conclusión:**\n",
        "El modelo palabra a palabra gestiona mejor las narrativas en fragmentos largos.\n",
        "\n",
        "\n",
        "**Estilo**\n",
        "--\n",
        "El modelo palabra a palabra no logra capturar la forma de poesía, el texto lo de vuelve como toda una oración, no devuelve los saltos de línea como si lo hace el modelo carácter a carácter."
      ],
      "metadata": {
        "id": "bkjrRGH5BVdM"
      }
    }
  ]
}