{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "L2Hx3VrxiM0v"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2"
      ],
      "metadata": {
        "id": "HmPxslriaLBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente problema, se presenta un conjunto de datos correspondientes a escritos de Shakespear. El objetivo del problema es crear un modelo capaz de generar texto con dialecto de época y escritura en verso y prosa."
      ],
      "metadata": {
        "id": "LM9LGmdGf6S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerías y entorno"
      ],
      "metadata": {
        "id": "LQDQfTDHght4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow 2.15\n",
        "!pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMk3TDzUVbzc",
        "outputId": "cbbf80bf-dfa8-46e1-cf9f-537309242b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRaa1rfYfHrK",
        "outputId": "d1a9d751-b68d-4bd2-b56c-601bbd29e277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from googletrans import Translator\n"
      ],
      "metadata": {
        "id": "2GnYMdQPgk7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()"
      ],
      "metadata": {
        "id": "c1aq1OznfQV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar para que TensorFlow utilice la GPU por defecto\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Configurar para que TensorFlow asigne memoria dinámicamente\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        # Especificar la GPU por defecto\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Manejar error\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "tvt87nZ6hl-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41730dc2-5474-400a-e0e4-9c5db0a54572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "dPvq7eUmhqgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVSjpQPGffP3",
        "outputId": "8c427bad-8cbc-4f14-8bbf-d1bb2215602a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descarga completada.\n"
          ]
        }
      ],
      "source": [
        "# URL del dataset\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "\n",
        "# Realizar la solicitud de descarga\n",
        "response = requests.get(url)\n",
        "\n",
        "# Guardar el archivo\n",
        "with open(\"shakespeare.txt\", \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(\"Descarga completada.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(\"shakespeare.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlpPa_JAhcaL",
        "outputId": "fcd03e73-699c-4cb9-e2c4-db4221cb8ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 2000 characters in text\n",
        "print(text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh_HvXOshvfL",
        "outputId": "3b2ce0f4-ec1d-4d8f-d198-6977d8610231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbGQT5cQhx50",
        "outputId": "b7779fba-dc3b-40db-caff-ef20e69932af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo carácter a carácter"
      ],
      "metadata": {
        "id": "qXfJPqUJZ6O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento"
      ],
      "metadata": {
        "id": "RtabpoUciKSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorización"
      ],
      "metadata": {
        "id": "L2Hx3VrxiM0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previo al entrenamiento, necesitamos convertir el texto a una representacion numerica.\n",
        "\n",
        "La capa tf.keras.layers.StringLookup nos permite convertir cada caracter en un ID numerico. Solo necesita que el texto este separado primero en tokens.\n",
        "\n",
        "A la par de hacerlo para nuestro texto lo mostramos con un ejemplo, para entender cómo funciona"
      ],
      "metadata": {
        "id": "ZJ9FHbQhiQ3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rZ1RNAliVGA",
        "outputId": "823d3e88-410e-401f-80c3-82cabb7d6e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora creamos la capa tf.keras.layers.StringLookup:"
      ],
      "metadata": {
        "id": "G3fbfbthiXHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "NKMGaya6iVur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos convierte de tokens a IDs de caracteres:"
      ],
      "metadata": {
        "id": "FUuzbon4ifYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao-m28x0ifLN",
        "outputId": "3d970d81-1773-4e1c-d749-c70564bc6bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que el proposito de este ejercicio es generar texto, tambien sera importante invertir esta representacion y recuperar texto legible desde estos IDs. Para esto utilizamos `tf.keras.layers.StringLookup(..., invert=True).`\n",
        "\n",
        "Nota: Aquí, en lugar de pasar el vocabulario original generado con `sorted(set(text))`, usamos el método `get_vocabulary()` de la capa `tf.keras.layers.StringLookup` para que los tokens `[UNK]` se configuren de la misma manera."
      ],
      "metadata": {
        "id": "J9MJXrecij_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "uWtBS7u8ipTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta capa recupera los caracteres desde los vectores de IDs y los retorna como un `tf.RaggedTensor` de caracteres:\n",
        "\n"
      ],
      "metadata": {
        "id": "iOEeQXMEiq7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Jt5Bu1isq1",
        "outputId": "7b5c0f84-b31d-4f92-9645-daccd99a3da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente usando `tf.strings.reduce_join` se pueden volver a juntar los caracteres en texto."
      ],
      "metadata": {
        "id": "yKU08XcuivoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfKSN05Piw7B",
        "outputId": "2e847ba0-fbd4-47aa-8e66-8f1e07f383c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "IrgdHsN1iz5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos el texto en ids numéricos\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipJx5M7tYqMU",
        "outputId": "eb81f5a3-3ed4-48d3-c1c1-60ed60393d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "MWOMmBI3YsFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eKtJpjZYvCt",
        "outputId": "fbe03637-8f26-4053-b32a-57eacf60e39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción carácter a carácter\n",
        "\n",
        "\n",
        "Dado un caracter, o una secuencia de caracteres, ¿cuál es el siguiente caracter más probable? Esta es la tarea para la que estamos entrenando al modelo. La entrada al modelo será una secuencia de caracteres y entrenamos el modelo para predecir la salida: el siguiente carácter en cada paso de tiempo.\n",
        "\n",
        "Dado que los RNN mantienen un estado interno que depende de los elementos vistos anteriormente, a partir de todos los caracteres calculados hasta este momento, ¿cuál es el siguiente carácter?"
      ],
      "metadata": {
        "id": "XVOFU5DFi3c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear los ejemplos de entrenamiento\n",
        "\n",
        "Dividimos el texto en secuencias de ejemplo. Cada secuencia de entrada contendrá `seq_length` caracteres del texto.\n",
        "\n",
        "Para cada secuencia de entrada, los targets correspondientes contienen la misma longitud de texto, excepto que se desplazan un carácter hacia la derecha.\n",
        "\n",
        "Así que divida el texto en fragmentos de `seq_length+1`. Por ejemplo, digamos `que seq_length` es 3 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hol\" y la secuencia target \"ola\".\n",
        "\n",
        "Para hacer esto, usamos la función `tf.data.Dataset.from_tensor_slices` para convertir el vector de texto en una secuencia de índices de caracteres."
      ],
      "metadata": {
        "id": "e2FetSLejSWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "yP19-GoLja3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método `batch` nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado.\n",
        "\n"
      ],
      "metadata": {
        "id": "CbQf6NCLjdO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeVmQtC5jfHQ",
        "outputId": "8425708b-eb9d-4f4d-adf8-478402a95b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es mas facil ver lo que esta haciendo si unimos de vuelta los tokens en texto:"
      ],
      "metadata": {
        "id": "n4Fwj_3gjixc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3HBX96rjhkk",
        "outputId": "cc87c4ec-4720-40f6-9b27-3e98ead1fb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el entrenamiento, necesitaremos un conjunto de datos de pares `(input, label)`. Donde `input` y `label` son secuencias. En cada timestep, la entrada es el carácter actual y la etiqueta es el siguiente carácter.\n",
        "\n",
        "Aquí hay una función que toma una secuencia como entrada, la duplica y la desplaza para alinear la entrada y la etiqueta para cada timestep:"
      ],
      "metadata": {
        "id": "xFUd5l16jnjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "92Zo4tvIjnMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos el método de arriba\n",
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccAAxwsZjqoA",
        "outputId": "21300372-bb8b-41e9-926d-3e33164b20be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "UlnUzZBkjsIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXpeYymLjtIf",
        "outputId": "0cf9114f-fbec-45af-9694-a88fc155c725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batches de entrenamiento"
      ],
      "metadata": {
        "id": "BMV2Jepcj5Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos `tf.data` para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, es necesario mezclarlos y batchearlos."
      ],
      "metadata": {
        "id": "0C08AkCoj_w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgn9pU-Rj40C",
        "outputId": "c3c12a3a-2004-454d-8ca3-c5edb89930f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construccion del modelo\n",
        "\n",
        "\n",
        "Este modelo tiene tres capas:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: La capa de entrada. Una lookup table entrenable que asignará cada ID de carácter a un vector con dimensiones `embedding_dim`;\n",
        "* `tf.keras.layers.GRU`: una capa recurrente GRU de tamaño units=rnn_units (también se puede usar una capa LSTM aquí).\n",
        "* `tf.keras.layers.Dense`: La capa de salida, con salidas` vocab_size`. Genera un logit para cada carácter del vocabulario. Estas son las probabilidades de cada caracter según el modelo."
      ],
      "metadata": {
        "id": "is38o59lkEFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "9mVaflZYkLH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "YoE96w00kNNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "3cXZQsp5kPC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por cada caracter el modelo calcula su embedding, corre la GRU un timestep con el embedding como entrada y aplica la capa densa para generar los logits prediciendo la probabilidades del siguiente caracter."
      ],
      "metadata": {
        "id": "dGZ9YCZkkjZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probar el modelo\n",
        "\n",
        "Ejecutamos el modelo para ver que se comporta como se esperaba.\n",
        "\n",
        "Primero verificamos la shape de salida:"
      ],
      "metadata": {
        "id": "MMwHUDrQkoe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht3JUN0ykiw9",
        "outputId": "bcc14907-70e9-4ad9-bc88-d8e1729e0791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejemplo anterior, la longitud de la secuencia de la entrada es 100, pero el modelo se puede ejecutar con entradas de cualquier longitud:"
      ],
      "metadata": {
        "id": "NehQq2FFku7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fpP2A43kuRz",
        "outputId": "aafe954a-58b1-4bb6-8d7b-1cb73f8d0853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para obtener predicciones reales del modelo, se deben tomar muestras de la distribución de salida para obtener índices de caracteres reales. Esta distribución está definida por los logits sobre el vocabulario de los caracteres.\n",
        "\n",
        "Nota: Es importante tomar una muestra de esta distribución, ya que tomar el argmax de la distribución puede fácilmente hacer que el modelo se atasque en un bucle.\n",
        "\n",
        "Tomando como ejemplo el primero del batch:"
      ],
      "metadata": {
        "id": "eoeyrEz5kydf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "b95l1tOPk0F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos da para cada timestep una predicción del siguiente índice de caracteres:\n",
        "\n"
      ],
      "metadata": {
        "id": "nrMdexg4nMnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R66EvZVonNun",
        "outputId": "0a569302-3732-451b-dab2-5c0fd222bbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 26, 57, 23, 27, 12,  2, 50, 58, 65, 44, 22,  6, 34, 42, 42,  6,\n",
              "        2, 61, 11, 11, 15, 65, 31, 22, 30, 39,  2,  8, 65, 24, 34, 46, 16,\n",
              "       25, 13, 46, 64, 32, 56,  3, 60, 65, 14, 31, 21,  2, 58, 42, 11, 26,\n",
              "       43, 31, 23, 23, 20, 31, 46, 58, 60, 51, 46, 40, 57, 47, 42, 63, 12,\n",
              "       55,  4, 37, 11,  5, 37, 60, 52, 16, 46, 54, 19, 42, 60, 25,  1, 65,\n",
              "        3, 12, 35, 41, 40, 53, 12, 43,  2,  4,  1, 23, 27, 63, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDzXBilMoR7j",
        "outputId": "c23e79eb-dba5-4c5f-b0fb-e76cb04dd969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"bring not comfort home,\\nThey'll give him death by inches.\\n\\nSICINIUS:\\nWhat's the news?\\n\\nSecond Messen\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"[UNK]MrJN; kszeI'Ucc' v::BzRIQZ -zKUgCL?gySq!uzARH sc:MdRJJGRgsulgarhcx;p$X:&XumCgoFcuL\\nz!;Vban;d $\\nJNxz\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "El problema puede tratarse como un problema de clasificación estándar. Dado el estado RNN anterior y la entrada en este timestep, predice la clase del siguiente carácter.\n"
      ],
      "metadata": {
        "id": "kw5ZlVtIob9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Agregamos un optimizador y una funcion costo**\n",
        "\n",
        "La función de pérdida estándar `tf.keras.losses.sparse_categorical_crossentropy` funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
        "\n",
        "Debido a que su modelo devuelve logits, necesita configurar el indicador `from_logits`."
      ],
      "metadata": {
        "id": "I3i-p3PEbERo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "id": "FWNuzPqjobuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "id": "7jnisWXqolb6",
        "outputId": "942e304a-e42a-4682-976c-c895ac23d2aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190633, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo recién inicializado no debería estar demasiado seguro de sí mismo, todos los logits de salida deberían tener magnitudes similares. Para confirmar esto, puede comprobar que la exponencial del costo medio es aproximadamente igual al tamaño del vocabulario. Una pérdida mucho mayor significa que el modelo está seguro de sus respuestas incorrectas y está mal inicializado:"
      ],
      "metadata": {
        "id": "LgCLUMoubS-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr2No2FpbUGi",
        "outputId": "41303fa9-aaf8-4051-956b-a3c8eb16e46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.06458"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos el modelo con `tf.keras.Model.compile` indicando el optimizador y la funcion costo:\n",
        "\n"
      ],
      "metadata": {
        "id": "8fo1BV4LbWc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "67r3M9hKbWEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoints del modelo**"
      ],
      "metadata": {
        "id": "SnEk5zVDbhrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos el callback `tf.keras.callbacks.ModelCheckpoint` para que se guarden checkpoints del modelo durante el entrenamiento."
      ],
      "metadata": {
        "id": "OTg6MiVccSwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "0TSvqyQNbce5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejecucion del entrenamiento**"
      ],
      "metadata": {
        "id": "PvM3EAr-cWcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero entrenamos con 20 épocas, al no obtener buenos resultos, decimos utilizar 40.\n",
        "\n",
        "En el entrenamiento con 20 épocas notamos que el texto no era coherente."
      ],
      "metadata": {
        "id": "Vz-Uh1V0cYCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40"
      ],
      "metadata": {
        "id": "wXZGh9tOcZz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n"
      ],
      "metadata": {
        "id": "dNF_LWTRgjs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmTBQfVvYiru",
        "outputId": "9b3e8fa5-35fa-458d-8d76-487554ee0764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 2.7212\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.9940\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.7153\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 1.5545\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.4540\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.3865\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 1.3346\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.2896\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.2489\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.2095\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.1688\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.1282\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.0848\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.0396\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.9908\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.9403\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.8880\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 16s 75ms/step - loss: 0.8358\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.7845\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.7358\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.6895\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.6509\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.6128\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.5850\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.5586\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 0.5361\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.5165\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.5009\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 0.4873\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4758\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4673\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.4579\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 15s 79ms/step - loss: 0.4537\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.4455\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4400\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.4377\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4325\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.4288\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.4287\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generacion de texto"
      ],
      "metadata": {
        "id": "ehQCTn5hcdk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La forma más sencilla de generar texto con este modelo es ejecutarlo en un bucle y realizar un seguimiento del estado interno del modelo a medida que lo ejecutamos.\n",
        "\n"
      ],
      "metadata": {
        "id": "0XEMqJ5jciqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada vez que llamamos al modelo, pasamos algún texto y un estado interno. El modelo devuelve una predicción para el siguiente caracter y su nuevo estado. Vuelva a pasar la predicción y el estado para continuar generando texto.\n",
        "\n",
        "\n",
        "Lo siguiente hace una predicción de un solo paso:"
      ],
      "metadata": {
        "id": "Pmxx-nvnck3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "6dwUVnTtccq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "EfeIXkqqdHZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo ejecutamos en un bucle para generar texto. Al observar el texto generado, veremos que el modelo sabe cuándo poner mayúsculas, hacer párrafos e imita un vocabulario de escritura similar al de Shakespear. Con el reducido número de épocas de entrenamiento, todavía no ha aprendido a formar frases coherentes, incluso primero probamos con 20 épocas y luego con 40 y notamos el mismo compartamiento."
      ],
      "metadata": {
        "id": "ByXqYgsDdJPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Caius Marcius'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI38ZqQ5dJDA",
        "outputId": "6d023b91-ec31-4a05-e22b-937f544f5c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caius Marcius?\n",
            "\n",
            "BRUTUS:\n",
            "It stands your grace, why should we fight?\n",
            "Where is thy lapour need no means mailest us;\n",
            "Me then we will open them. My cousin Juliet,\n",
            "To lusting not himself with shried and entering\n",
            "banish'd; though unskilf my budgets, care not so.\n",
            "\n",
            "MENENIUS:\n",
            "The son, whereof I give you, sir, thanks, and\n",
            "Tread the King of heaven, the tongues droposors!\n",
            "Where are thy tongue climbs us not one but death:\n",
            "And when I give me leave to retire,\n",
            "Or by the honourable roots be courted\n",
            "A sentence of your breast for bearing but\n",
            "Yelk in 's east, if they were equals;\n",
            "'Twere all other forbid be a happy death:\n",
            "'Tis bastard keen's; the tunes of death no puties for sense;\n",
            "They tend the cause of my body's voices\n",
            "Are now we can make heavy gnam; it will come to me,\n",
            "In the ear that is about a lady, your\n",
            "night, lords, going to find him stealth by:\n",
            "but I had given me back the case of France\n",
            "Hath pawnet out an open in our gentle highwes;\n",
            "So far in blood, thou, idle weeds and kneel\n",
            "but my death with child; and live wi \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 12.179988384246826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder interpretar mejor los resultados obtenidos, lo traducimos al español. La idea es poder analizar si el texto generado tiene coherencia."
      ],
      "metadata": {
        "id": "l3QXkYukfis2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traducción al español\n",
        "generated_text = result[0].numpy().decode('utf-8')\n",
        "translated_text = translator.translate(generated_text, src='en', dest='es').text\n",
        "print(\"Texto en español:\\n\")\n",
        "print(translated_text, '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgjExUiBfgG2",
        "outputId": "438a59d9-fe1c-4f3f-d2ee-86f085a766fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto en español:\n",
            "\n",
            "¿Caius Marcius?\n",
            "\n",
            "Brutus:\n",
            "Era su nombre.Asalto\n",
            "De Roma;'Es pero la alondra mañana\n",
            "Y todas las cosas que pueden contemplarlos con mi corazón,\n",
            "Sobre tu fatal cannertome empinado\n",
            "Eso los hizo en la parte de su sonido obitiano:\n",
            "Prefiero creo que este terreno\n",
            "Como yo por confesado contra mi descanso\n",
            "Mientras que me ponen a todos:\n",
            "También lo es la envidia en la luna.\n",
            "\n",
            "Northumberland:\n",
            "¿Cuál fue tu deber para la muerte?¡Oh pecado!\n",
            "¿Qué te da tanto ahora, con un poderoso tortillero que deberíamos?\n",
            "Me desatan a otro.\n",
            "\n",
            "Marcius:\n",
            "Si el medio noble pudiera morir.\n",
            "\n",
            "York:\n",
            "Tengamos tu pompa, Malk;Su nombre es Tybalt, tú, mis señores,\n",
            "Si la pena me puede hacer por sí misma, rico en desprecio,\n",
            "Y te saqueó, acomoda a Juliut un mundo.Alack, digo.\n",
            "\n",
            "Enfermero:\n",
            "¡Amante!¡Oh!Estamos malcriados y la velocidad.\n",
            "\n",
            "Pedante:\n",
            "Jurar tú?Art tú eres mentira, en cuestión,\n",
            "Ir a whipph;y, por el campo de Saint Alban,\n",
            "La verdadera esperanza se apresuró al dolor de encuentro\n",
            "Para desatrar su madre murió: Porque no he tenido más\n",
            "de casto, si son nuevos castillos;\n",
            "Dibujar con rencor inactivo nunca c \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además de aumentar las épocas también podríamos experimentar con una secuencia de inicio diferente, intentar agregar otra capa RNN para mejorar la precisión del modelo o ajustar el parámetro de temperatura para generar predicciones más o menos aleatorias.\n",
        "\n",
        "Veremos si luego del modelo para predecir palabra a palabra agregamos algo de todo esto."
      ],
      "metadata": {
        "id": "7JlkLf67dMcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos que el modelo genere texto *más rápido*, lo más fácil que se puede hacer es generar el texto por batches. En el siguiente ejemplo, el modelo genera 5 resultados aproximadamente en el mismo tiempo que tomó generar 1 arriba."
      ],
      "metadata": {
        "id": "hTsztEtbdOiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Caius Marcius', 'Caius Marcius', 'Caius Marcius', 'Caius Marcius', 'Caius Marcius'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dsmP_1ddQEs",
        "outputId": "bc32759f-80bb-4a85-9fa4-10c0439ec220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"Caius Marcius?\\n\\nHORTENSIO:\\nMadam, where lies Clubb'd with a bond?\\n\\nHASTINGS:\\nMy lord!\\n\\nKING RICHARD III:\\nSweet Kate, no, not the son o' the midst\\nJetuen her fanth. Conceive a brother!\\n\\nISABELLA:\\nWhy do you come?\\n\\nBoatswain:\\nLook, you three, Margaret, Romeo was broke:\\nWhich grieves next was a forterler.\\n\\nKING RICHARD III:\\nStanley, look upon thee.\\n\\nForrecesain:\\nI pray now, keep you uncannot be:\\nO, she, your father tell me what thou learn'd,\\nThat they are in a treaty. Time, lead ashame!\\nI pray, sir, we have writ you?\\n\\nSecond Murderer:\\n'Zounds, whom thou liest; his noble cousins may prove\\nSomething hither come to see him and all chaff:\\nDreaming suddensicle thirtus, which at\\nlasting rebellion; and then indeed pluck\\nhath made a poor cenvay for a king:\\nCome, bravE Old nothing but sweeter's death,\\n'I would here perceive this dead!\\n\\nGLOUCESTER:\\nNor no one that do sit down.\\n\\nKING RICHARD III:\\nSoldiers shall this body to my reputation.\\nMake all the vawsage of thy chamber me?\\n\\nGREMIO:\\n\\nThird Messenger:\\nLook, w\"\n",
            " b\"Caius Marcius?\\n\\nCOMINIUS:\\nI am a-weary, give;\\nNo better urge so long.\\nAs leanns should be in revenge;\\nThough I with kinsman do assure thee, at thy foot\\nWere new so bittle; 'tis trees--\\nThat it was wont to think on eaten and earth\\nMight through cagual toposour, his, whilst on Warwick\\non the field of Griefory is my house,\\nTo us the titings of York,\\nAnd all the measure of your speech,\\nI cannot do.\\n\\nISABAL:\\nRelenting, banish! Why do me no, for I\\nreign accusers; and the tyrant.\\n\\nPETRUCHIO:\\nSignior Baptista; follow me,\\nSince I have knowlest his household, Officer,\\nOr this deop'd presently exempt here!\\n\\nDUKE OF AUMERLE:\\nFor ever may grief, which to your good worship\\nTo eack, rejoir them and mercy butcher,\\nTo miserop our locks in heartily.\\n\\nQUEEN ELIZABETH:\\nFirst you what you sin?\\n\\numba, that end what you do and his wife's consent or oil\\nTo thin upon our apposed Aunio.\\n\\nLEONTES:\\nDo, Plantagenet! if the duke you know\\nthe cause wry not the stone rebuke me\\nFrom manigh'st alliance; that bear thee over his\\npres\"\n",
            " b\"Caius Marcius?\\n\\nCOMINIUS:\\nI think 'twers of Naples,\\nHe waxed wear out gentle and to cozen themselves\\nTo conquer all those fathers, and they\\nsetted up about the strong, and on our knees\\nTo ong him soundly fair and secondings.\\n\\nThird Servingman:\\n'Tis a life, which is the best of them, do not\\nUpbraid'st from Claudio! There's no more remains;\\nBut first, though I could wish me from them;\\nYet not so be slain; I'll find them turn:\\nFor never, now those cheers will prophesy is out.\\n\\nDUKE VINCENTIO:\\nSirrah, cousin!\\n\\nPRINCE EDWAR:\\nAs God defend me not, my brother Duke of York;\\nAnd now reside thee here; then all alouned\\nStarks me of his country, and rid his face;\\nFor thou must sue; the sit doth ever speak too whence,\\nThou talk'st of nature loved her kinsmen face;\\nAbout his nobleness, whom we see madice; which\\nI will not promise to do it.\\nWhat, fair musician, amout?\\n\\nROMEO:\\nYour plaises, of what I have done a homan throne,\\nAs over-tweinsed. The marshalt form.\\n\\nFLORIZEL:\\nIt then be not tongue.\\n\\nPETRUCHIO:\\nTwo, t\"\n",
            " b\"Caius Marcius?\\n\\nBRUTUS:\\nConsider at my lady's face,\\nBidemable, with thee my friends,'s wife, three,\\nThou camest to branch of mine,\\nAnd overment to do anothersely.\\n\\nISABELLA:\\nAy, if the murderer let me bear?\\n\\nPOMPEY:\\nI bear the ship so many hearing when commandman Sheigning Isabel,\\nEven to the heart that are their tunes.\\n\\nMayord:\\nHis raise himself with child, and banished;\\nFor thou hadst sent home, and he sleep commanded:\\nWilt thou resort to-morrow?\\n\\nI must two good nurse,\\nI speak asleepier than the wholes, marshalls at once\\nshe bedieve me. A herself, is there a markin\\nWoo hath importund't watch'd in his wick?\\n\\nPETER:\\nI swear to the English queen's again.\\n\\nCOMINIUS:\\nLet's about it.\\n\\nANGELO:\\nWere not well served in the number of my sons,\\nIf false boy of the apparel to do lose thee, and\\nprepert them; and now for that wise men but\\nWith all the hope enough to many sons,\\nOf something the lower more comes in our man\\nUntine to be in my daughter's daughter:\\nLood, they are soft and accupt, my friends make pi\"\n",
            " b\"Caius Marcius?\\n\\nANGELO:\\nNay, but his revengess prayer.\\nMy cantle-rable course of joy,\\nStir and bred his disposition; hid with\\nwenched him and cheque, as if they have ever\\nstoned that were the sea that I said angel honest.\\n\\nRICHMOND:\\nIf with ride and perhaps; I saw him for a Verce,\\nTell me for this fought tragit with her sceptre\\n'say her rough, thou shouldst not be;\\nAnd thou canst no living look,\\nAnd by the broken report of beaving in\\nThe secret infusion of the people,\\nThe shapes of beasts discover'd, that ever ta'e thine eye.\\nOr shall we tell them who is left why then lived.\\n\\nLEONTES:\\nI'll ha'th a pale.\\n\\nROMEO:\\nWell, see our spirits o'erther\\n'Tis sentenced. Thou for a feodate man, I on't,\\nAnd but a long and weeds I see,\\nIf ever knew like thee that thou sounds\\nThat three daints were flowing with our soldiers\\nhave robs conceived that news, and the Volscian names,\\nBreak autumn'd condition, have chasteps\\nyour pleasure here; I know she is a past man's nepe,\\nAnd to that end with child.\\n\\nLUCIO:\\nSome speed\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 12.370666980743408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del modelo carácter a carácter"
      ],
      "metadata": {
        "id": "d14fbklXdnX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate=500, temperature=1.0):\n",
        "    input_eval = ids_from_chars(tf.strings.unicode_split(start_string, 'UTF-8')).numpy()\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(chars_from_ids(predicted_id).numpy().decode('utf-8'))\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "gnztj3EjdglD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionamos diferentes temperaturas y longitudes para generar los textos"
      ],
      "metadata": {
        "id": "wBzmiVrPdysG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar fragmentos con diferentes configuraciones\n",
        "start_string = \"Caius Marcius\"\n",
        "\n",
        "# Fragmentos con temperatura = 1\n",
        "print(\"Temperatura 1:\")\n",
        "for i in range(5):\n",
        "    print(f\"Fragmento {i+1}:\")\n",
        "    print(generate_text(model, start_string=start_string, num_generate=200, temperature=1.0))\n",
        "    print()\n",
        "\n",
        "# Fragmentos con temperatura < 1\n",
        "print(\"Temperatura < 1 (0.5):\")\n",
        "for i in range(5):\n",
        "    print(f\"Fragmento {i+1}:\")\n",
        "    print(generate_text(model, start_string=start_string, num_generate=200, temperature=0.5))\n",
        "    print()\n",
        "\n",
        "# Fragmentos con temperatura > 1\n",
        "print(\"Temperatura > 1 (1.5):\")\n",
        "for i in range(5):\n",
        "    print(f\"Fragmento {i+1}:\")\n",
        "    print(generate_text(model, start_string=start_string, num_generate=200, temperature=1.5))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gtrvKrEdzA5",
        "outputId": "7ea66baf-3562-4763-e0c4-8f400a80f098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperatura 1:\n",
            "Fragmento 1:\n",
            "Caius Marcius?\n",
            "O:\n",
            "Why bonco'd yous ur ber INIO haf tees y ichthtowhoushousthin,\n",
            "Horer'th lioourn d\n",
            "I'derg ak ar gllelf cu s s bora ange sck thend imous\n",
            "Pam s\n",
            "CA:\n",
            "\n",
            "LAs veathestourtuthiedoour t ngererdor yow ayourst\n",
            "\n",
            "Fragmento 2:\n",
            "Caius Marcius?\n",
            "SUSINTowe\n",
            "VO:\n",
            "\n",
            "\n",
            "Houlourthas, pren inena me, nd, whiend se abrs pranorthige, asive, h peirear'TELely,\n",
            "Loupliverd in oungomily s;\n",
            "Whithouckes thicheree,\n",
            "Why.\n",
            "CHiowin t th m.\n",
            "\n",
            "\n",
            "\n",
            "F:\n",
            "ME Detus youst apel \n",
            "\n",
            "Fragmento 3:\n",
            "Caius Marcius?\n",
            "VE:\n",
            "CEn d pree man,\n",
            "\n",
            "Londs smamyouthitovedese! urad I byouce w culds h blele bl s cl thesis, wh, s by s dst w ause wimanor me;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I f de toue thor theyofor;\n",
            "Hee se, he t\n",
            "\n",
            "Thal har aten paly w h why\n",
            "\n",
            "Fragmento 4:\n",
            "Caius Marcius?\n",
            "Chith,\n",
            "ORI housamoutes t\n",
            "Thomy w ar!\n",
            "NINTher,\n",
            "HORKI t thouthinowe tharkistst bred tlete, thin t, tee s HE:\n",
            "\n",
            "\n",
            "Pl-viseno bre llethey----ck I ivin g thit,\n",
            "\n",
            "HESe th buthisp' ll thizers; h athare,\n",
            "\n",
            "TE ho\n",
            "\n",
            "Fragmento 5:\n",
            "Caius Marcius?\n",
            "\n",
            "Thed, banour thecondile.\n",
            "I mais, LINCowif whimy mang'dyor afonend ishanave.\n",
            "Cort Am.\n",
            "Tor thino fomoulown busay thoure:\n",
            "\n",
            "Tu s, thas Prok le'd uthtot lllllll br or' h thew,\n",
            "Vil;\n",
            "\n",
            "OMICHAMy, ist, t t a\n",
            "\n",
            "Temperatura < 1 (0.5):\n",
            "Fragmento 1:\n",
            "Caius Marcius?\n",
            "An thithan d he bll thous f our an th thare s thar' y w an d th ththe the th t be thour halllle moure mame mas thor in our nd s thaly, he in howingourousen alimo ns the ther tho w the s th thathe in\n",
            "\n",
            "Fragmento 2:\n",
            "Caius Marcius? ar f t the h thonour s th t wer thallly hen th than s byour an, he thour thel our thean an the s thee ce ur angour t hes han pe haththered ow the thonshe f the hand w whoure ureris blllar I thy that\n",
            "\n",
            "Fragmento 3:\n",
            "Caius Marcius?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TE:\n",
            "Thoutoman, the ingond, the f y ar t s thour s then the thoure cour the che the mand f wher four's pres thater thither thar t thang whe hathan thin the tharth th th hathe hind gherar the cer \n",
            "\n",
            "Fragmento 4:\n",
            "Caius Marcius?\n",
            "\n",
            "\n",
            "\n",
            "And thar the ar t ithour d t thowiswayo the we thoour in the aprend thinthee thitous thesour hand tharers ar t thes t thous thes arere thallind s the me thanoura and hout than than the thar's d t\n",
            "\n",
            "Fragmento 5:\n",
            "Caius Marcius?\n",
            "VOFoure tho thour ar'd thind hour m th tous t t tho thayour wand the t our athar thar d p ouen se then INCh tharethes wand the br the ant thands isousousther tofingre thindow th ansoure ar s t tinge\n",
            "\n",
            "Temperatura > 1 (1.5):\n",
            "Fragmento 1:\n",
            "Caius Marcius?\n",
            "heyou; nily.\n",
            "Wh ay h,\n",
            "I\n",
            "Wely IOvoumseaiththt EORUCin tinicur,\n",
            "Whiviet tan s f dolly,\n",
            "WETEguliooull' fare trtowh?\n",
            "ININGHasin whe: Ina th vame; Cingrrsir; glyo tet Sio;\n",
            "T:\n",
            "\n",
            "KIOLowntoBEded, h favais, p\n",
            "\n",
            "Fragmento 2:\n",
            "Caius Marcius?\n",
            "Ct:\n",
            "l ld w. m Bulen, ing otethandy.\n",
            "FYor RY:\n",
            "As, uraive ig n:\n",
            "NULINAvevin orthe? hy torarsthu o sele; miees ld,\n",
            "O:\n",
            "Weapivotof hey ss BULeincigoseve mer that;\n",
            "UCor th!\n",
            "F flald, se t aitot\n",
            "YORCOUCUYO:\n",
            "\n",
            "Fragmento 3:\n",
            "Caius Marcius? choudodsthak'\n",
            "Cio cige:\n",
            "HOUTINCHAfours:\n",
            "Luifuies.\n",
            "An thailanou\n",
            "[UNK]lleadsunw worsit:\n",
            "\n",
            "BRI'sw st'sopr Fimyor.\n",
            "Plf polilll pimy, me Jof t sw O d w'skst t d tuestis. cierotet o-mys RARO&se, MO: IUMan, cal\n",
            "\n",
            "Fragmento 4:\n",
            "Caius Marcius?\n",
            "Whtofrgh meinf ou f f, ce terd measce veill I'Aseses d d.\n",
            "Ren le\n",
            "Prashescidss wit\n",
            "UCJUThanety th,\n",
            "HOFooness y.\n",
            "Tet chinon, s\n",
            "NGe'ly'dof ng htsth h coly olBY my\n",
            "\n",
            "CAngugondave? lig theninourrdes h,\n",
            "LE\n",
            "\n",
            "Fragmento 5:\n",
            "Caius Marcius?\n",
            "S:\n",
            "NINCLithor Isonte' brdur'd hnad ousix cenditarow ICKE:\n",
            "\n",
            "RGRE f agung ucugstupodousmay hoocr IUCivend ak berond t, amathoprero hthy:\n",
            "Tist g s\n",
            "\n",
            "ARES BE; y, ay GA:\n",
            "mofe;\n",
            "Mo,\n",
            "GRin'l wnse Hoch h couto\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones:\n",
        "* Temperatura = 1.0\n",
        "\n",
        "Genera texto con una mezcla moderada de creatividad y coherencia.\n",
        "Las frases tienen cierta estructura gramatical pero contienen muchas palabras inventadas o incoherentes.\n",
        "\n",
        "* Temperatura < 1.0 (0.5)\n",
        "\n",
        "Las predicciones se vuelven más conservadoras. El texto tiende a repetir patrones comunes y generar frases más predecibles y monótonas.\n",
        "Hay menos creatividad y se observan repeticiones frecuentes de palabras o estructuras.\n",
        "Se observa muy poca choerencia en el texto generado.\n",
        "\n",
        "* Temperatura > 1.0 (1.5)\n",
        "\n",
        "Aumenta considerablemente la diversidad en el texto, pero a costa de la coherencia.\n",
        "Se producen palabras y estructuras altamente aleatorias, que no tienen sentido.\n",
        "La salida resulta caótica y menos útil para aplicaciones prácticas donde la coherencia es esencial.\n",
        "\n",
        "En algunos casos podemos ver que el texto conserva mayúsculas donde corresponden y la forma de verso y prosa."
      ],
      "metadata": {
        "id": "nt91gTMlg7Vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probamos distintas longitudes de secuencia"
      ],
      "metadata": {
        "id": "DODaWaGQeDf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos con longitudes diferentes\n",
        "longitudes = [100, 200, 500]\n",
        "\n",
        "for length in longitudes:\n",
        "    print(f\"Fragmento con longitud {length}:\")\n",
        "    print(generate_text(model, start_string=start_string, num_generate=length, temperature=1.0))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbbHWuMPeGNz",
        "outputId": "aee9484a-b02f-4ac4-897d-3a23c8e94566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fragmento con longitud 100:\n",
            "Caius Marcius?\n",
            "Stho! touresit I hour oores f se ar alame, the ve;\n",
            "\n",
            "Lougnd bers t ff a doust I e shatoucour d, thi\n",
            "\n",
            "Fragmento con longitud 200:\n",
            "Caius Marcius?\n",
            "He' re the;\n",
            "\n",
            "Honkeceindoun s n he bu\n",
            "Chtonf orof mach t be! sthy bly, hureengees winof shand p nsowin:\n",
            "Whepr, belllay grave w ous d; s amenghave Car--pr me mase t, IIEnthathomyousery Gll u uro anses\n",
            "\n",
            "Fragmento con longitud 500:\n",
            "Caius Marcius?\n",
            "Tous here t.\n",
            "HO:\n",
            "fer th t Pe the berece thosor,\n",
            "SAnoficu nglls ystathasivivindour h bllll nd.\n",
            "CEO:\n",
            "CEThonou'dagustely.\n",
            "S beve bopad cken melotar'sifoupllerd oured vea thopoug Starse, thilloveas,\n",
            "ARINGHARelss, bucensirde:\n",
            "D couthesurou IN: f a byst, w qurajur me:\n",
            "ICHat ds goromanther couryoschaismecigovofanthyoupr pours, hy gr winchers;\n",
            "INThan t s, this; icord sass\n",
            "CEOMy m coflll igmuroulinte wis h me aswnca llld ang, f CHare Pr?\n",
            "\n",
            "NEgee htower lld bl-lithtuto aviestha haferst bourou nd tho be p\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fragmento con longitud 100:\n",
        "Observación: El texto es breve y presenta fragmentos de palabras y frases que se acercan más a un estilo coherente. La falta de contexto suficiente hace que no logre desarrollar una narrativa sólida.\n",
        "\n",
        "Ejemplo destacado:\n",
        "\n",
        "**\"Caius Marcius? Stho! touresit I hour oores f se ar alame, the ve;\"**\n",
        "\n",
        "Aunque las palabras no tienen sentido completo, el uso de puntuación y estructura tiene cierta similitud con un diálogo dramático.\n",
        "\n",
        "\n",
        "* Fragmento con longitud 200:\n",
        "Aparecen más palabras conectadas y estructuras similares a frases de Sheaskpear. Sin embargo, al aumentar la longitud, el modelo comienza a introducir incoherencias más evidentes.\n",
        "\n",
        "Ejemplo destacado:\n",
        "\n",
        "**\"Whepr, belllay grave w ous d; s amenghave Car--pr me mase t\"**\n",
        "\n",
        "Aunque incoherente, se observan patrones del estilo del texto de entramiento, como palabras truncadas y una estructura similar a un poema.\n",
        "\n",
        "* Fragmento con longitud 500:\n",
        " Aunque logra mantener cierta consistencia estilística (uso de puntuación, extructura de poema), la narrativa se desarma rápidamente, y las incoherencias se vuelven evidentes.\n",
        "\n",
        "Ejemplo destacado:\n",
        "\n",
        "**\"Tous here t. HO: fer th t Pe the berece thosor, SAnoficu nglls ystathasivivindour h bllll nd.\"**\n",
        "El texto carece de sentido, pero mantiene un estilo que podría parecer inspirado en un diálogo dramático.\n",
        "\n",
        "**Conclusión general:** A mayores longitudes, el modelo pierde su capacidad de mantener una estructura coherente, y las repeticiones, palabras inventadas y frases ininteligibles se incrementan.\n"
      ],
      "metadata": {
        "id": "hsEh5p-Sh2DI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo palabra a palabra"
      ],
      "metadata": {
        "id": "hXuHYrgpjCJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorización"
      ],
      "metadata": {
        "id": "J5JQ8WlTjEgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cambia de carácter a carácter, a palabra a palabra:"
      ],
      "metadata": {
        "id": "QFWabEctnBNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()  # Divide el texto en palabras usando espacios\n",
        "print(f\"Cantidad de palabras: {len(words)}\")\n",
        "print(f\"Primeras 10 palabras: {words[:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PV619yEm27Y",
        "outputId": "13b80d30-b3a1-44f4-84f5-9ebecd9c2e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de palabras: 202651\n",
            "Primeras 10 palabras: ['First', 'Citizen:', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapeamos las palabras a IDs"
      ],
      "metadata": {
        "id": "7T2E1363nXUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])  # Aprende el vocabulario\n",
        "word_index = tokenizer.word_index  # Diccionario de palabras a IDs\n",
        "sequences = tokenizer.texts_to_sequences([text])  # Convierte el texto a secuencias de IDs\n",
        "sequences = sequences[0]  # Extrae la lista de IDs, ya que `texts_to_sequences` devuelve una lista de listas\n",
        "\n",
        "print(f\"Primeras 10 IDs: {sequences[:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ656hMBnZH2",
        "outputId": "0a8b7db1-6321-4f95-e53b-756d700da642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 10 IDs: [88, 269, 139, 35, 969, 143, 668, 127, 15, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos a dataset\n",
        "dataset_2 = tf.data.Dataset.from_tensor_slices(sequences)"
      ],
      "metadata": {
        "id": "tW9pY7wcncb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción"
      ],
      "metadata": {
        "id": "5_Oiy_5OjP8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividimos las secuencias en entrada y salida (target):"
      ],
      "metadata": {
        "id": "H2mK1ULNnnCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 10  # Longitud de cada secuencia\n",
        "sequences = dataset_2.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]  # Todo menos el último\n",
        "    target_text = chunk[1:]  # Todo menos el primero\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset_2 = sequences.map(split_input_target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bNrOLy_nkwj",
        "outputId": "ec38cc7c-bc68-4696-be49-6d9d86762a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset_2 = dataset_2.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "w_xcxD8enufV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamos el modelo palabra a palabra"
      ],
      "metadata": {
        "id": "TgDxTpobn8Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        # Embedding layer ensures 3D tensor (batch_size, seq_length, embedding_dim)\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "m6k0bbuRl8NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_index) + 1  # Incluye 1 para el índice 0 (padding, si se usa)\n",
        "embedding_dim = 256  # Dimensión del embedding (puedes ajustar según el experimento)\n",
        "rnn_units = 1024  # Número de unidades en la GRU (también ajustable)\n",
        "\n",
        "model_2 = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")\n"
      ],
      "metadata": {
        "id": "oNeCTtUGkn4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "id": "FIDAtkMnom6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "VqThzkbQopB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for input_example, target_example in dataset_2.take(1):\n",
        "#     print(f\"Entrada (IDs): {input_example.numpy()}\")\n",
        "#     print(f\"Objetivo (IDs): {target_example.numpy()}\")"
      ],
      "metadata": {
        "id": "Iy5V3r23iNLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(optimizer='adam', loss=loss)\n",
        "history = model_2.fit(dataset_2, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGK0ISkpjrli",
        "outputId": "8871620d-1ab4-45a9-81c6-71bf4b64400c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "289/289 [==============================] - 17s 53ms/step - loss: 7.0909\n",
            "Epoch 2/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 6.3032\n",
            "Epoch 3/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 5.8748\n",
            "Epoch 4/20\n",
            "289/289 [==============================] - 16s 54ms/step - loss: 5.4771\n",
            "Epoch 5/20\n",
            "289/289 [==============================] - 15s 52ms/step - loss: 5.0090\n",
            "Epoch 6/20\n",
            "289/289 [==============================] - 16s 54ms/step - loss: 4.4585\n",
            "Epoch 7/20\n",
            "289/289 [==============================] - 15s 52ms/step - loss: 3.8888\n",
            "Epoch 8/20\n",
            "289/289 [==============================] - 15s 52ms/step - loss: 3.3593\n",
            "Epoch 9/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 2.8779\n",
            "Epoch 10/20\n",
            "289/289 [==============================] - 15s 52ms/step - loss: 2.4419\n",
            "Epoch 11/20\n",
            "289/289 [==============================] - 16s 52ms/step - loss: 2.0601\n",
            "Epoch 12/20\n",
            "289/289 [==============================] - 16s 52ms/step - loss: 1.7350\n",
            "Epoch 13/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 1.4689\n",
            "Epoch 14/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 1.2570\n",
            "Epoch 15/20\n",
            "289/289 [==============================] - 16s 52ms/step - loss: 1.0889\n",
            "Epoch 16/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 0.9613\n",
            "Epoch 17/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 0.8625\n",
            "Epoch 18/20\n",
            "289/289 [==============================] - 16s 53ms/step - loss: 0.7834\n",
            "Epoch 19/20\n",
            "289/289 [==============================] - 16s 52ms/step - loss: 0.7238\n",
            "Epoch 20/20\n",
            "289/289 [==============================] - 19s 65ms/step - loss: 0.6763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_seed(seed_text, tokenizer, seq_length):\n",
        "    # Convierte las palabras de la semilla en IDs\n",
        "    seed_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    # Ajusta la longitud agregando ceros al inicio si es necesario\n",
        "    seed_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [seed_seq], maxlen=seq_length, padding='pre'\n",
        "    )\n",
        "    return seed_seq\n"
      ],
      "metadata": {
        "id": "ZTZvKG6AoP4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, seq_length, num_words, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "    seed_seq = prepare_seed(seed_text, tokenizer, seq_length)\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        # Predice las probabilidades de la siguiente palabra\n",
        "        predictions = model(seed_seq, training=False)\n",
        "        predictions = predictions[:, -1, :]  # Obtén las predicciones de la última palabra\n",
        "\n",
        "        # Ajusta las probabilidades usando la temperatura\n",
        "        predictions = tf.nn.softmax(predictions / temperature).numpy()\n",
        "\n",
        "        # Selecciona la siguiente palabra basada en las probabilidades\n",
        "        next_word_id = np.random.choice(len(predictions[0]), p=predictions[0])\n",
        "        next_word = tokenizer.index_word.get(next_word_id, \"<unk>\")\n",
        "\n",
        "        # Agrega la palabra generada al texto\n",
        "        generated_text += \" \" + next_word\n",
        "\n",
        "        # Actualiza la semilla\n",
        "        seed_seq = np.append(seed_seq[0], next_word_id)[-seq_length:]\n",
        "        seed_seq = np.expand_dims(seed_seq, axis=0)\n",
        "\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "NhezbLJnoHZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"To be or not to be\"\n",
        "generated = generate_text(\n",
        "    model=model_2,\n",
        "    tokenizer=tokenizer,\n",
        "    seed_text=seed_text,\n",
        "    seq_length=10,  # Debe coincidir con `seq_length` usado en el entrenamiento\n",
        "    num_words=50,   # Número de palabras a generar\n",
        "    temperature=1.0 # Experimenta con valores <1 (más conservador) o >1 (más creativo)\n",
        ")\n",
        "print(\"Texto generado:\")\n",
        "print(generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG1CsJ1moJCp",
        "outputId": "49a29be4-1c62-47c5-a3cb-153f108055a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto generado:\n",
            "To be or not to be his valiant on one person runs but sore himself on him or my brother if that knew your grace i'll speak that cold lord king henry vi peace for the king my mind is an heir that went my lord by once and wilt this cut from my end by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated = translator.translate(generated, src='en', dest='es').text\n",
        "\n",
        "# Mostrar texto traducido\n",
        "print(\"\\nTexto traducido al español:\")\n",
        "print(translated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch_5l0Z1lMLr",
        "outputId": "719d6b79-15b4-47c7-808d-c0f65537171c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texto traducido al español:\n",
            "Ser o no ser su valiente en una persona corre, pero se adhiere a él o a mi hermano si eso supiera tu gracia, hablaré ese frío Señor Rey Henry VI Paz para el Rey Mi mente es un heredero que fue mi Señor una vezy marchitar este corte de mi final por\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso podemos ver que, aunque el texto generado no es perfectamente lógico, las palabras están organizadas en frases que tienen una estructura gramatical razonable en varios tramos. Esto indica que el modelo ha captado patrones frecuentes en el texto de entrenamiento.\n",
        "\n",
        " Algunas partes del texto tienen sentido ambiguo o parecen fuera de contexto, como \"runs but sore himself on him\" o \"an heir that went my lord by once\". Esto refleja que el modelo aún tiene dificultades para generar contenido con sentido narrativo completo.\n",
        "\n",
        " El texto no se genera en forma de verso, como si lo esta el texto de entramiento.\n"
      ],
      "metadata": {
        "id": "dmpwSBlxlgNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluamos el modelo"
      ],
      "metadata": {
        "id": "g2JcPqdNoidF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multiple_fragments(model, tokenizer, seq_length, num_words, num_fragments, temperature):\n",
        "    fragments = []\n",
        "    for _ in range(num_fragments):\n",
        "        # Generar una semilla aleatoria de palabras\n",
        "        random_seed = \" \".join(np.random.choice(list(tokenizer.word_index.keys()), size=seq_length))\n",
        "        # Generar texto basado en la semilla\n",
        "        generated_text = generate_text(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            seed_text=random_seed,\n",
        "            seq_length=seq_length,\n",
        "            num_words=num_words,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        fragments.append(generated_text)\n",
        "    return fragments\n"
      ],
      "metadata": {
        "id": "2PSvuDB9omMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar fragmentos al azar\n",
        "num_fragments = 5\n",
        "num_words = 50\n",
        "seq_length = 10\n",
        "temperature = 1.0  # Puedes ajustar según el análisis solicitado\n",
        "\n",
        "# Fragmentos para modelo palabra a palabra\n",
        "fragments_word_to_word = generate_multiple_fragments(\n",
        "    model=model_2,\n",
        "    tokenizer=tokenizer,\n",
        "    seq_length=seq_length,\n",
        "    num_words=num_words,\n",
        "    num_fragments=num_fragments,\n",
        "    temperature=temperature\n",
        ")\n",
        "\n",
        "print(\"Fragmentos generados (Palabra a palabra):\")\n",
        "for i, fragment in enumerate(fragments_word_to_word, 1):\n",
        "    print(f\"Fragmento {i}:\\n{fragment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBOFBRRjooRn",
        "outputId": "30f35aa4-1890-4ab2-9b97-5ddb74f082b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fragmentos generados (Palabra a palabra):\n",
            "Fragmento 1:\n",
            "owe corslet fellow'st afeard aiding wonderful cleaving reigning purgatory confounds the course hastings and montague that will be piece of thee take on thee in thy and cross and bring we make deadly this second senator help back away and we'll aid thee here who lost there this a prince mine for thou art an executioner king richard iii farewell\n",
            "\n",
            "Fragmento 2:\n",
            "valance two bides lineal catcher beginning miseries follower tinkers bluntly we'll have the most of them 3 king bolingbroke so many a french crown to conquest to tell it how should not he spoke first citizen he cannot come menenius if you do i but by your grace by my poor complaint late he should for your art king lewis\n",
            "\n",
            "Fragmento 3:\n",
            "star lordship sojourn ensnareth masquing wholesomest myself immaculate methought cures in their nicely like an easy fills and maids by one royal blind to our daughter and be obedient well then thou art a villain romeo what am the wench that juliet is so young so would never be katharina by he or a second man's as one at strange\n",
            "\n",
            "Fragmento 4:\n",
            "grow died viewing white ague's pawn'd costly assured unshunned spirit for shame comes to his name between you and his consent isabella approach i to be pity by no man king richard iii a devil and hear me make that for her dead that i myself love a day in marriage my the life hath got my face and with\n",
            "\n",
            "Fragmento 5:\n",
            "prattling axe attending bale interruption grasshoppers wasted hide blinds library that i have fought with those course i come unto thee tyrrel bring us my noble kiss king richard ii resign as we be sworn god then we might go to him and twice free by points into their other sight by fight each other we would say to each\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a evaluarlo modificando la temperatura y la longitud de secuencia"
      ],
      "metadata": {
        "id": "fbso8CUHmTDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(\n",
        "    model, tokenizer, seed_text, seq_length, num_words, temperature=1.0\n",
        "):\n",
        "    \"\"\"Genera texto palabra a palabra con un modelo entrenado.\"\"\"\n",
        "    input_text = seed_text\n",
        "    for _ in range(num_words):\n",
        "        # Tokenización y preprocesamiento\n",
        "        token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        token_list = token_list[-seq_length:]  # Usar solo los últimos `seq_length` tokens\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [token_list], maxlen=seq_length, padding='pre'\n",
        "        )\n",
        "\n",
        "        # Predicción\n",
        "        predictions = model.predict(token_list, verbose=0)\n",
        "\n",
        "        # Select the logits for the next word prediction (last timestep)\n",
        "        predictions = predictions[:, -1, :]  # Reshape to [1, vocabulary_size]\n",
        "\n",
        "        predictions = predictions / temperature  # Ajustar temperatura\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Decodificar la palabra predicha\n",
        "        output_word = tokenizer.index_word.get(predicted_id, \"\")\n",
        "        if output_word == \"\":  # Si no se encuentra, detener\n",
        "            break\n",
        "        input_text += \" \" + output_word\n",
        "\n",
        "    return input_text\n",
        "\n",
        "# Parámetros iniciales\n",
        "seed_text = \"To be or not to be\"\n",
        "seq_lengths = [5, 10, 20]  # Diferentes longitudes de secuencia\n",
        "temperatures = [0.5, 1.0, 1.5]  # Diferentes temperaturas\n",
        "num_words = 50  # Palabras generadas\n",
        "\n",
        "# Evaluar combinaciones\n",
        "for seq_length in seq_lengths:\n",
        "    print(f\"\\n=== Evaluando longitud de secuencia: {seq_length} ===\")\n",
        "    for temp in temperatures:\n",
        "        print(f\"\\n** Temperatura: {temp} **\")\n",
        "        generated_text = generate_text(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            seed_text=seed_text,\n",
        "            seq_length=seq_length,\n",
        "            num_words=num_words,\n",
        "            temperature=temp\n",
        "        )\n",
        "        print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JLBQuKgmkju",
        "outputId": "fee932d3-0478-44e9-ecbf-c3bf4522a320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluando longitud de secuencia: 5 ===\n",
            "\n",
            "** Temperatura: 0.5 **\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be and shall from are how you they and are good now is the him are was was come are good now is and she from now was now and come they and from now which o and how at and do now and from now was and now then at\n",
            "\n",
            "** Temperatura: 1.0 **\n",
            "To be or not to be and thy well how was are do which now that and it at more now they and are how and from are here now and on was now now and how from their and lord well or now and come how for the the but are at which now was\n",
            "\n",
            "** Temperatura: 1.5 **\n",
            "To be or not to be and shall with by shall he it as is the with from and they now now and from now was and lord come now to the the thy well they they at then to the the but by king him he what is the thou at was lord and your\n",
            "\n",
            "=== Evaluando longitud de secuencia: 10 ===\n",
            "\n",
            "** Temperatura: 0.5 **\n",
            "To be or not to be and he they and how from now was now and from are here now and he and lord now then their and how from now now and come how and do their and how from now and they well come how at was come now they and do which at\n",
            "\n",
            "** Temperatura: 1.0 **\n",
            "To be or not to be and he they and how from now was now and their at well and on which now lord my the thy which are their and from at she and how from at well and thee are then they how my the all from well they and he and thee are\n",
            "\n",
            "** Temperatura: 1.5 **\n",
            "To be or not to be and have well be come so if is the be come they thee from are was good now and your lord she are was lord my and how are or now and how from at well and from come how from now was for and shall from are how and\n",
            "\n",
            "=== Evaluando longitud de secuencia: 20 ===\n",
            "\n",
            "** Temperatura: 0.5 **\n",
            "To be or not to be and shall with by shall he it as is the all was well now my and it which come on on at was lord not and how from now then and from are lord and which now on how and thee at more on at was how and more now\n",
            "\n",
            "** Temperatura: 1.0 **\n",
            "To be or not to be the shall come how from are which my and she now and more well they how and now then thee at well then how now was and at on and how from now and they from come o the all at and how from now and o was come then\n",
            "\n",
            "** Temperatura: 1.5 **\n",
            "To be or not to be and your thou he our with me your all this is the all at and so at was on at which or my and she from at they now a lord are their and come they and on at well then lord not the me well how and they from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones:**\n",
        "\n",
        "* Temperatura = 0.5:\n",
        "\n",
        "El modelo tiende a ser más conservador, eligiendo palabras con alta probabilidad en lugar de explorar opciones menos frecuentes.\n",
        "Esto resulta en texto repetitivo, con frases como \"and from now\" o \"now and how\" repitiéndose en múltiples instancias.\n",
        "La coherencia general es moderada, pero el texto carece de variedad y creatividad.\n",
        "\n",
        "* Temperatura = 1.0:\n",
        "\n",
        "Se alcanza un balance entre creatividad y coherencia.\n",
        "Hay más variedad en las palabras generadas, aunque algunas combinaciones no tienen sentido completo.\n",
        "\n",
        "* Temperatura = 1.5:\n",
        "\n",
        "La generación se vuelve muy creativa pero menos coherente.\n",
        "Frases poco comprensibles.\n",
        "\n",
        "* Secuencia = 5:\n",
        "\n",
        "La información de contexto es limitada, lo que provoca un texto más repetitivo y menos contextualizado.\n",
        "Frases como \"and are good now\" se repiten con poca relación entre las palabras generadas.\n",
        "\n",
        "* Secuencia = 10:\n",
        "\n",
        "El modelo tiene un contexto más amplio, lo que mejora la coherencia del texto.\n",
        "Se observa mayor diversidad, pero aún persisten repeticiones y falta de una narrativa clara.\n",
        "\n",
        "* Secuencia = 20:\n",
        "\n",
        "Un contexto más largo permite al modelo generar frases más variadas y conectadas.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "La combinación de temperatura = 1.0 y una longitud de secuencia mayor (10 o 20) produce los textos más equilibrados, con cierta diversidad y un nivel aceptable de coherencia.\n",
        "Temperatura alta (>1.0) puede ser útil para explorar creatividad, pero genera frases menos comprensibles.\n",
        "\n"
      ],
      "metadata": {
        "id": "--90RDRem9f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparación y conclusiones sobre ambos modelos"
      ],
      "metadata": {
        "id": "_alYSDYLn_iP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo Carácter a Carácter:**\n",
        "\n",
        "**Ventajas:**\n",
        "* Sigue patrones detallados a nivel de\n",
        "caracteres, incluyendo ortografía, puntuación y estilo.\n",
        "* Útil para generar el texto en forma de verso.\n",
        "\n",
        "**Desventajas:**\n",
        "* Puede generar palabras inexistentes o ininteligibles debido a la falta de una comprensión semántica más amplia.\n",
        "* Coherencia limitada en frases largas, ya que no tiene un concepto explícito de \"palabra\".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Modelo Palabra a Palabra:**\n",
        "\n",
        "**Ventajas:**\n",
        "* Genera texto con mayor coherencia y sentido lógico.\n",
        "* Es menos propenso a errores de ortografía.\n",
        "\n",
        "**Desventajas:**\n",
        "* No genera el texto siguiendo la forma de verso.\n",
        "* No conserva patrones literarios.\n",
        "\n",
        "---\n",
        "\n",
        "**Creatividad:**\n",
        "* Carácter a Carácter:\n",
        "Muestra creatividad en las combinaciones de letras y patrones inesperados.\n",
        "Sin embargo, esta creatividad puede llevar a incoherencia y palabras inventadas.\n",
        "* Palabra a Palabra:\n",
        "Genera textos más predecibles, aunque todavía variados, especialmente con temperaturas más altas.\n",
        "Produce combinaciones menos arriesgadas pero más comprensibles.\n",
        "---\n",
        "\n",
        "**Coherencia:**\n",
        "* Carácter a Carácter:\n",
        "Frases cortas pueden ser coherentes, pero se pierde sentido en secuencias más largas debido a la falta de semántica.\n",
        "Con temperatura = 1.0, logra el mejor equilibrio entre creatividad y sentido.\n",
        "\n",
        "* Palabra a Palabra:\n",
        "Tiene una clara ventaja en coherencia. Incluso con secuencias largas, los textos tienen mayor lógica.\n",
        "---\n",
        "**Repetición:**\n",
        "* Carácter a Carácter:\n",
        "Tiende a repetir patrones y estructuras a nivel de letras o sílabas.\n",
        "* Palabra a Palabra:\n",
        "Puede repetir palabras, pero la repetición es menos frecuente.\n",
        "---\n",
        "**Parámetros:**\n",
        "* Carácter a Carácter:\n",
        "Muy sensible a la longitud de secuencia: secuencias más largas generan texto más fluido, aunque con riesgos de incoherencia.\n",
        "La temperatura influye mucho en la creatividad, pero temperaturas muy altas tienden a producir texto caótico.\n",
        "* Palabra a Palabra:\n",
        "La longitud de secuencia mejora la capacidad de mantener contexto, pero incluso con secuencias cortas genera textos aceptables.\n",
        "La temperatura afecta la diversidad de palabras pero no compromete tanto la coherencia.\n",
        "\n",
        "---\n",
        "**Conclusión General**\n",
        "\n",
        "Si el objetivo es la creatividad o textos que imiten un patrón específico, el modelo carácter a carácter es más adecuado, aunque requiere ajustes para evitar incoherencias.\n",
        "Si el objetivo es generar textos comprensibles y coherentes el modelo palabra a palabra es claramente superior..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60ku4PGGoJ37"
      }
    }
  ]
}